/* User. (C) 2005-2010 Niall Douglas

Boost Software License - Version 1.0 - August 17th, 2003

Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:

The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
*/

#ifdef ENABLE_USERMODEPAGEALLOCATOR

#include "nedtrie.h"

#ifndef PAGE_SIZE
#define PAGE_SIZE 4096
#endif

#define REGION_ENTRY(type)                        NEDTRIE_ENTRY(type)
#define REGION_HEAD(name, type)                   NEDTRIE_HEAD(name, type)
#define REGION_INIT(treevar)                      NEDTRIE_INIT(treevar)
#define REGION_EMPTY(treevar)                     NEDTRIE_EMPTY(treevar)
#define REGION_GENERATE(proto, treetype, nodetype, link, cmpfunct) NEDTRIE_GENERATE(proto, treetype, nodetype, link, cmpfunct, NEDTRIE_NOBBLEZEROS(treetype))
#define REGION_INSERT(treetype, treevar, node)    NEDTRIE_INSERT(treetype, treevar, node)
#define REGION_REMOVE(treetype, treevar, node)    NEDTRIE_REMOVE(treetype, treevar, node)
#define REGION_FIND(treetype, treevar, node)      NEDTRIE_FIND(treetype, treevar, node)
#define REGION_NFIND(treetype, treevar, node)     NEDTRIE_NFIND(treetype, treevar, node)
#define REGION_MAX(treetype, treevar)             NEDTRIE_MAX(treetype, treevar)
#define REGION_MIN(treetype, treevar)             NEDTRIE_MIN(treetype, treevar)
#define REGION_NEXT(treetype, treevar, node)      NEDTRIE_NEXT(treetype, treevar, node)
#define REGION_PREV(treetype, treevar, node)      NEDTRIE_PREV(treetype, treevar, node)
#define REGION_FOREACH(var, treetype, treevar)    NEDTRIE_FOREACH(var, treetype, treevar)
#define REGION_HASNODEHEADER(treevar, node, link) NEDTRIE_HASNODEHEADER(treevar, node, link)

typedef struct region_node_s region_node_t;
struct region_node_s {
    REGION_ENTRY(region_node_s) linkS; /* by start addr */
    REGION_ENTRY(region_node_s) linkE; /* by end addr */
    REGION_ENTRY(region_node_s) linkL; /* by length */
    void *start, *end;
};
typedef struct regionS_tree_s regionS_tree_t;
REGION_HEAD(regionS_tree_s, region_node_s);
typedef struct regionE_tree_s regionE_tree_t;
REGION_HEAD(regionE_tree_s, region_node_s);
typedef struct regionL_tree_s regionL_tree_t;
REGION_HEAD(regionL_tree_s, region_node_s);

static size_t regionkeyS(const region_node_t *RESTRICT r)
{
  return (size_t) r->start;
}
static size_t regionkeyE(const region_node_t *RESTRICT r)
{
  return (size_t) r->end;
}
static size_t regionkeyL(const region_node_t *RESTRICT r)
{
  return (size_t) r->end - (size_t) r->start;
}
REGION_GENERATE(static, regionS_tree_s, region_node_s, linkS, regionkeyS);
REGION_GENERATE(static, regionE_tree_s, region_node_s, linkE, regionkeyE);
REGION_GENERATE(static, regionL_tree_s, region_node_s, linkL, regionkeyL);
static regionS_tree_t regiontreeS; /* The list of free regions, keyed by start addr */
static regionE_tree_t regiontreeE; /* The list of free regions, keyed by end addr */
static regionL_tree_t regiontreeL; /* The list of free regions, keyed by length */

#ifndef WIN32
typedef size_t PageFrameType;

/* This function determines whether the host OS allows user mode physical memory
page mapping. */
static int OSDeterminePhysicalPageSupport(void);

/* This function could ask the host OS for address space, or on embedded systems
it could simply parcel out space via moving a pointer. The second two void *
are some arbitrary extra data to be later passed to OSReleaseAddrSpace(). */
static void*[3] OSReserveAddrSpace(size_t space);

/* This function returns address space previously allocated using
OSReserveAddrSpace(). It is guaranteed to exactly match what was previously
returned by that function. */
static int OSReleaseAddrSpace(void *addr, size_t space, void *data[2]);

/* This function obtains physical memory pages, either by asking the host OS
or on embedded systems by simply pulling them from a free page ring list. */
static size_t OSObtainMemoryPages(PageFrameType *buffer, size_t number, void *data[2]);

/* This function returns previously obtained physical memory pages. */
static size_t OSReleaseMemoryPages(PageFrameType *buffer, size_t number, void *data[2]);

/* This function causes the specified range of physical memory pages to be
mapped at the specified range of addresses. On an embedded system this would
simply modify the MMU and flush the appropriate TLB entries. It works like this:

for(PageFrameType *pfa=*pageframes; *pagesperaddr && (pfa=*pageframes); addrs++, pagesperaddr++, pageframes++) {
  for(size_t page=0; page<*pagesperaddr; page++) {
    if(pfa[addr])
      Map(addr[page], pfa[page]);
    else
      Unmap(addr[page]);
  }
}
*/
static size_t OSRemapMemoryPagesOntoAddrs(void **addrs, size_t *pagesperaddr, PageFrameType **pageframes, void *data[2]);
#else
static enum {
  DISABLEEVERYTHING=1,
  HAVEPHYSICALPAGESUPPORT=4,
  NOPHYSICALPAGESUPPORT=5
} PhysicalPageSupport;

/* Windows has the curious problem of using 4Kb pages but requiring those pages
to be mapped at 64Kb aligned address. By far the easiest solution is to pretend
that we actually have 64Kb pages. */
#undef PAGE_SIZE
#define PAGE_SIZE 65536
typedef struct PageFrameType_t
{
  ULONG_PTR pages[16];
} PageFrameType;


static int OSDeterminePhysicalPageSupport(void)
{
  if(!PhysicalPageSupport)
  { /* Quick test */
    PageFrameType pageframe;
    size_t no=sizeof(PageFrameType)/sizeof(ULONG_PTR);
    SYSTEM_INFO si={0};
    if(AllocateUserPhysicalPages((HANDLE)-1, &no, &pageframe.pages))
    {
      FreeUserPhysicalPages((HANDLE)-1, &no, &pageframe.pages);
      PhysicalPageSupport=HAVEPHYSICALPAGESUPPORT;
    }
    else
      PhysicalPageSupport=NOPHYSICALPAGESUPPORT;
    GetSystemInfo(&si);
    if(si.dwAllocationGranularity!=PAGE_SIZE)
    {
      assert(si.dwAllocationGranularity==PAGE_SIZE);
      fprintf(stderr, "User Mode Page Allocator: Allocation granularity is %u not %u. Please recompile with corrected PAGE_SIZE\n", si.dwAllocationGranularity, PAGE_SIZE);
      PhysicalPageSupport=DISABLEEVERYTHING;
    }
    if(si.dwAllocationGranularity/si.dwPageSize!=sizeof(PageFrameType)/sizeof(ULONG_PTR))
    {
      assert(si.dwAllocationGranularity/si.dwPageSize==sizeof(PageFrameType)/sizeof(ULONG_PTR));
      fprintf(stderr, "User Mode Page Allocator: Pages per PageFrameType is %u not %u. Please recompile with corrected PageFrameType definition\n", si.dwAllocationGranularity/si.dwPageSize, sizeof(PageFrameType)/sizeof(ULONG_PTR));
      PhysicalPageSupport=DISABLEEVERYTHING;
    }
  }
  return PhysicalPageSupport;
}
static void*[3] OSReserveAddrSpace(size_t space)
{
  void *ret[3]= {0,0,0};
  if(!PhysicalPageSupport) OSDeterminePhysicalPageSupport();
  if(DISABLEEVERYTHING==PhysicalPageSupport) return ret;
  if(HAVEPHYSICALPAGESUPPORT==PhysicalPageSupport)
  {
    ret[0]=VirtualAlloc(NULL, space, MEM_RESERVE|MEM_PHYSICAL, PAGE_READWRITE);
  }
  if(!ret[0])
  {
    HANDLE fmh;
    fmh = CreateFileMapping(INVALID_HANDLE_VALUE, NULL, PAGE_READWRITE|SEC_RESERVE,
#if defined(_M_IA64) || defined(_M_X64) || defined(WIN64)
                            (DWORD)(space>>32),
#else
                            0,
#endif
                            (DWORD)(space&((DWORD)-1)), NULL);
    if(fmh)
    { /* This is breathtakingly inefficient, but win32 leaves us no choice :(
         At least this function is called very infrequently. */
      while((ret[0]=VirtualAlloc(NULL, space, MEM_RESERVE, PAGE_READWRITE)))
      {
        void *RESTRICT seg;
        VirtualFree(ret[0], 0, MEM_RELEASE);
        for(seg=ret[0]; seg<(void*)((size_t) ret[0] + space); seg=(void *)((size_t) seg + Win32granularity))
        {
          if(!VirtualAlloc(seg, Win32granularity, MEM_RESERVE, PAGE_READWRITE))
            break;
        }
        if(seg==(void*)((size_t) ret[0] + space))
          break;
        else
        {
          for(; seg>=ret[0]; seg=(void *)((size_t) seg - Win32granularity))
            VirtualFree(seg, 0, MEM_RELEASE);
        }
      }
      if(!ret[0])
        CloseHandle(fmh);
      else
      {
        ret[1]=(void *) fmh;
        ret[2]=(void *)(size_t) 1;
      }
    }
  }
  return ret;
}
static int OSReleaseAddrSpace(void *addr, size_t space, void *data[2])
{
  if(!data[0])
    return VirtualFree(addr[0], 0, MEM_RELEASE);
  else
  {
    void *seg;
    CloseHandle((HANDLE)data[0]);
    for(seg=addr; seg<(void*)((size_t) addr + space); seg=(void *)((size_t) seg + Win32granularity))
      VirtualFree(seg, 0, MEM_RELEASE);
    return 1;
  }
}
static size_t OSObtainMemoryPages(PageFrameType *RESTRICT buffer, size_t number, void *data[2])
{
  if(!data[0])
  {
    number*=sizeof(PageFrameType)/sizeof(ULONG_PTR);
    if(!AllocateUserPhysicalPages((HANDLE)-1, &number, buffer[0].pages))
      return 0;
    number/=sizeof(PageFrameType)/sizeof(ULONG_PTR);
  }
  else
  {
    size_t n, m;
    ULONG_PTR *RESTRICT pf=(ULONG_PTR *RESTRICT) &data[1];
    for(n=0; n<number; n++)
      for(m=0; m<sizeof(PageFrameType)/sizeof(ULONG_PTR); m++)
        buffer[n].pages[m]=(*pf)++;
    return number;
  }
  return 0;
}
static size_t OSReleaseMemoryPages(PageFrameType *RESTRICT buffer, size_t number, void *data[2])
{
  size_t n, m;
  if(!data[0])
  {
    number*=sizeof(PageFrameType)/sizeof(ULONG_PTR);
    if(!FreeUserPhysicalPages((HANDLE)-1, &number, buffer[0].pages)) return 0;
    number/=sizeof(PageFrameType)/sizeof(ULONG_PTR);
    for(n=0; n<number; n++)
      for(m=0; m<sizeof(PageFrameType)/sizeof(ULONG_PTR); m++)
        buffer[n].pages[m]=0;
    return number;
  }
  /* Always fail if we are emulating physical pages */
  return 0;
}
static size_t OSRemapMemoryPagesOntoAddrs(void *RESTRICT *addrs, PageFrameType *RESTRICT *RESTRICT pageframes, size_t *RESTRICT pagesperaddr, void *reservation, void *data[2])
{
  if(!data[0])
  {
    size_t *RESTRICT pages;
    for(pages=pagesperaddr; *pages; pages++)
      (*pages)*=sizeof(PageFrameType)/sizeof(ULONG_PTR);
    return MapUserPhysicalPagesScatter(addrs, pagesperaddr, pageframes);
  }
  else
  {
    size_t ret=1;
    PageFrameType *RESTRICT pfa, *RESTRICT pf;
    for(pfa=*pageframes; *pagesperaddr && (pfa=*pageframes); addrs++, pagesperaddr++, pageframes++)
    {
      size_t n;
      for(n=0, pf=pfa; pf<pfa+*pagesperaddr; n++, pf++)
      {
        void *addr=(void *)((size_t) *addr + n * PAGE_SIZE);
        if(*pf)
        {
          size_t filemappingoffset=PAGE_SIZE*((*pf)-1);
          /* Change reservation for next segment */
          if(!VirtualFree(addr, 0, MEM_RELEASE)) ret=0;
          if(!MapViewOfFileEx((HANDLE) data[0], FILE_MAP_ALL_ACCESS,
#if defined(_M_IA64) || defined(_M_X64) || defined(WIN64)
                             (DWORD)(filemappingoffset>>32),
#else
                             0,
#endif
                             (DWORD)(filemappingoffset & (DWORD)-1), PAGE_SIZE, addr)) ret=0;
        }
        else
        {
          if(!UnmapViewOfFile(addr)) ret=0;
          /* Change reservation for previous segment */
          if(!VirtualAlloc(addr, PAGE_SIZE, MEM_RESERVE, PAGE_READWRITE)) ret=0;
        }
      }
    }
    return ret;
  }
}

#endif

/* Maps an address reservation */
typedef struct AddressSpaceReservation_s AddressSpaceReservation_t;
static struct AddressSpaceReservation_s
{
  AddressSpaceReservation_t *next;
  void *front, *frontptr;         /* Grows upward */
  void *back, *backptr;           /* Grows downward */
  size_t pagesused;
  PageFrameType pagemapping[1];   /* Includes this structure */
} *addressspacereservation;

static AddressSpaceReservation_t *ReserveSpace(size_t space)
{
  const size_t RESERVEALWAYSLEAVEFREE=64*1024*1024; /* Windows goes seriously screwy if you take away all address space */
  void *addrR[3];
  AddressSpaceReservation_t *addr=0;
  size_t pagemappingsize, n, pagesallocated[2]={0,0};
  PageFrameType pagebuffer[256];
  if(space<(size_t)1<<30 /* 1Gb */)
  {
    space=(size_t)1<<30;
    if(8==sizeof(size_t)) space<<=2; /* Go for 4Gb chunks on 64 bit */
  }
  while(space>=RESERVEALWAYSLEAVEFREE && !*(addrR=OSReserveAddressSpace(space)))
    space>>=1;
  if(space<RESERVEALWAYSLEAVEFREE) return 0;
  pagemappingsize=sizeof(AddressSpaceReservation_t)+sizeof(PageFrameType)*((space/PAGE_SIZE)-2);
  pagemappingsize=(pagemappingsize+PAGE_SIZE-1) &~(PAGE_SIZE-1);
  pagemappingsize/=PAGE_SIZE;
  /* We now need pagemappingsize number of pages in order to store the mapping tables, but
  because this could be as much as 4Mb of stuff we'll need to do it in chunks to avoid
  breaking the stack. */
  for(n=0; n<pagemappingsize; n+=pagesallocated[0])
  {
    size_t torequest=sizeof(pagebuffer)/sizeof(PageFrameType);
    void *mapaddr=(void *)((size_t) addrR[0] + n*PAGE_SIZE);
    if(torequest>pagemappingsize-n) torequest=pagemappingsize-n;
    if(!(pagesallocated[0]=OSObtainMemoryPages(pagebuffer, torequest, addrR+1)))
      goto badexit;
    if(!OSRemapMemoryPagesOntoAddrs(&mapaddr, &pagebuffer, pagesallocated, addrR[0], addrR+1))
      goto badexit;
    if(!n)
    { /* This is the first run, so install AddressSpaceReservation */
      addr=(AddressSpaceReservation_t *) addrR[0];
      addr->front=addr->frontptr=(void *)((size_t)addr+pagemappingsize);
      addr->back=addr->backptr=(void *)((size_t)addr+space);
      addr->pagesused=0;
    }
    /* Add these new pages to the page mappings. Because we are premapping in new pages,
    we are guaranteed to have memory already there ready for us. */
    for(torequest=0; torequest<pagesallocated[0]; torequest++)
      addr->pagemapping[n+torequest]=pagebuffer[n];
  }
  return addr;
badexit:
  /* Firstly throw away any just allocated pages */
  if(pagesallocated[0])
    OSReleaseMemoryPages(pagebuffer, pagesallocated[0], addrR+1);
  if(addr)
  { /* Now throw away any previously stored */
    OSReleaseMemoryPages(addr->pagemapping, n, addrR+1);
  }
  OSReleaseAddrSpace(addrR[0], space, addrR+1);
  return 0;
}
static int CheckFreeAddressSpaces(AddressSpaceReservation_t **_addr)
{
  AddressSpaceReservation_t *addr=*_addr;
  if(!addr->next || CheckFreeAddressSpaces(&addr->next))
  {
    assert(!addr->next);
    if(0==addr->pagesused)
    {
      size_t size=(size_t)addr->back-(size_t)addr;
      assert(addr->frontptr==addr->front);
      assert(addr->backptr==addr->back);
      if(OSReleaseAddrSpace(addr, size))
      {
        *_addr=0;
        return 1;
      }
    }
  }
  return 0;
}
static void *AllocatePages(int fromback, size_t size)
{
  AddressSpaceReservation_t *addr;
  if(!addressspacereservation && !(addressspacereservation=ReserveSpace(0)))
  {
    fprintf(stderr, "Failed to allocate initial address space\n");
    abort();
  }
  for(addr=addressspacereservation; addr; addr=addr->next)
  {
    if((size_t) addr->backptr - (size_t) addr->frontptr>=size)
    {
      void *ret=fromback ? (void *)((size_t) addr->backptr - size) : addr->frontptr;
      PageFrameType *pagemappings=addr->pagemapping+((size_t)ret-(size_t)addr)/PAGE_SIZE;
      size_t n;
      for(n=0; n<size/PAGE_SIZE && *pagemappings; n++, pagemappings++);
      if(n<size/PAGE_SIZE)
      {
        void *memtocommit=(void *)((size_t) ret+n*PAGE_SIZE);
        size_t memtocommitsize=size-n*PAGE_SIZE;
        if(!fromback || VirtualAlloc(memtocommit, memtocommitsize, MEM_COMMIT, PAGE_READWRITE))
        {
          addr->pagesused+=memtocommitsize/PAGE_SIZE;
          for(; n<size/PAGE_SIZE; n++)
            *pagemappings++=(PageFrameType)(size_t)-1;
        }
      }
      if(n==size/PAGE_SIZE)
      {
        if(fromback)
          addr->backptr=(void *)((size_t) addr->backptr - size);
        else
          addr->frontptr=(void *)((size_t) addr->frontptr + size);
        return ret;
      }
    }
    if(!addr->next)
    {
      //DumpFreeRegions();
      addr->next=ReserveSpace((size_t) 1<<(nedtriebitscanr(size-1)+1));
    }
  }
  return 0;
}
static int ReleasePages(void *mem, size_t size)
{ /* Returns 1 if address space was freed */
  AddressSpaceReservation_t *addr;
  for(addr=addressspacereservation; addr; addr=addr->next)
  {
    if(mem>=addr->front && mem<addr->back)
    {
      int fromback=mem>=addr->backptr;
      if(mem>=addr->frontptr && mem<addr->backptr)
      {
        assert(0);
        abort();
      }
      /* There is a useful optimisation here for sysalloc of not dewiring the pages, just
      telling the OS that their contents are no longer important so the pages can be taken
      by another process needing them if needs be. mmapped regions are still decommitted
      and recommitted as we need their contents to be zero. */
      if(!fromback)
      {
        //VirtualAlloc(mem, size, MEM_RESET, PAGE_READWRITE);
      }
      else
      {
        size_t n;
        PageFrameType *pagemappings=addr->pagemapping+((size_t)mem-(size_t)addr)/PAGE_SIZE;
        VirtualFree(mem, size, MEM_DECOMMIT);
        for(n=0; n<size/PAGE_SIZE; n++)
          *pagemappings++=(PageFrameType)(size_t)0;
        addr->pagesused-=size/PAGE_SIZE;
      }
      if((size_t) addr->frontptr-size==(size_t) mem || (size_t) addr->backptr==(size_t) mem)
      {
        if(fromback)
          addr->backptr=(void *)((size_t) addr->backptr + size);
        else
          addr->frontptr=(void *)((size_t) addr->frontptr - size);
        //if(!addr->pagesused)
        //  CheckFreeAddressSpaces(&addressspacereservation);
        return 1;
      }
      return 0;
    }
  }
  return 0;
}

#define REGIONSTORAGESIZE (PAGE_SIZE*4)
#define REGIONSPERSTORAGE ((REGIONSTORAGESIZE-2*sizeof(void *))/sizeof(region_node_t))
typedef struct RegionStorage_s RegionStorage_t;
static struct RegionStorage_s
{
  RegionStorage_t *next;
  region_node_t *freeregions;
  region_node_t regions[REGIONSPERSTORAGE];
} *regionstorage;
static size_t regionsallocated, regionsfree;

static region_node_t *AllocateRegionNode(void)
{
  int n;
  if(!regionstorage || !regionstorage->freeregions)
  {
    RegionStorage_t **_rs=!regionstorage ? &regionstorage : &regionstorage->next, *rs;
    if(regionstorage) while(*_rs) _rs=&((*_rs)->next);
    assert(sizeof(RegionStorage_t)<=REGIONSTORAGESIZE);
    if((rs=*_rs=(RegionStorage_t *) AllocatePages(1, REGIONSTORAGESIZE)))
    {
      for(n=REGIONSPERSTORAGE-1; n>=0; n--)
      {
        *(region_node_t **)&rs->regions[n]=regionstorage->freeregions;
        regionstorage->freeregions=&rs->regions[n];
      }
      regionsallocated+=REGIONSPERSTORAGE;
      regionsfree+=REGIONSPERSTORAGE;
    }
  }
  if(regionstorage->freeregions)
  {
    region_node_t *ret=regionstorage->freeregions;
    regionstorage->freeregions=*(region_node_t **)ret;
    *(region_node_t **)ret=0;
    regionsfree--;
    return ret;
  }
  return 0;
}
static int CheckFreeRegionNodes(RegionStorage_t **_rs)
{
  if(regionsfree==regionsallocated)
  {
    if(!(*_rs)->next || CheckFreeRegionNodes(&(*_rs)->next))
    {
      assert(!(*_rs)->next);
      ReleasePages(*_rs, REGIONSTORAGESIZE);
      *_rs=0;
      regionsallocated=regionsfree=0;
      return 1;
    }
  }
  return 0;
}
static void FreeRegionNode(region_node_t *node)
{
  memset(node, 0, sizeof(region_node_t));
  *(region_node_t **)node=regionstorage->freeregions;
  regionstorage->freeregions=node;
  regionsfree++;
}


static void *userpage_malloc(size_t size, unsigned flags)
{
  region_node_t node, *r;
  /* Firstly find out if there is a free slot of sufficient size and if so use that.
  If there isn't a sufficient free slot, extend the virtual address space */
  if(size<PAGE_SIZE) size=PAGE_SIZE;
  size=(size+PAGE_SIZE-1)&~(PAGE_SIZE-1);
  node.start=0;
  node.end=(void *)size;
  r=REGION_NFIND(regionL_tree_s, &regiontreeL, &node);
  if(r)
  {
    void *ret=0;
    size_t rlen=(size_t) r->end - (size_t) r->start;
    if(rlen<size)
    {
      assert(rlen>=size);
    }
    REGION_REMOVE(regionS_tree_s, &regiontreeS, r);
    REGION_REMOVE(regionL_tree_s, &regiontreeL, r);
    if(rlen==size)
    { /* Release the node entirely */
      ret=r->start;
      REGION_REMOVE(regionE_tree_s, &regiontreeE, r);
      FreeRegionNode(r);
      return ret;
    }
    /* Reinsert r with new start addr */
    ret=r->start;
    r->start=(void *)((size_t) r->start + size);
    REGION_INSERT(regionS_tree_s, &regiontreeS, r);
    REGION_INSERT(regionL_tree_s, &regiontreeL, r);
    return ret;
  }
  else
  { /* Extend the VA allocation */
    void *ret=AllocatePages(0, size);
    return !ret ? MFAIL : ret;
  }
}

static int userpage_free(void *mem, size_t size)
{
  region_node_t node, *RESTRICT prev, *RESTRICT next, *RESTRICT r=0;
  /* Can I merge with adjacent free blocks? */
  if(size<PAGE_SIZE) size=PAGE_SIZE;
  size=(size+PAGE_SIZE-1)&~(PAGE_SIZE-1);
  node.start=(void *)((size_t)addr+size);
  node.end=addr;
  prev=REGION_FIND(regionE_tree_s, &regiontreeE, &node);
  next=REGION_FIND(regionS_tree_s, &regiontreeS, &node);
  node.start=addr;
  node.end=(void *)((size_t)addr+size);
  if(prev && next)
  { /* Consolidate into prev */
    assert(prev->end==node.start);
    assert(next->start==node.end);
    REGION_REMOVE(regionS_tree_s, &regiontreeS, next);
    REGION_REMOVE(regionE_tree_s, &regiontreeE, next);
    REGION_REMOVE(regionL_tree_s, &regiontreeL, next);
    node.end=next->end;
    FreeRegionNode(next);
  }
  if(prev)
  { /* Consolidate into prev */
    assert(prev->end==node.start);
    REGION_REMOVE(regionE_tree_s, &regiontreeE, prev);
    REGION_REMOVE(regionL_tree_s, &regiontreeL, prev);
    r=prev;
    r->end=node.end;
  }
  else if(next)
  { /* Consolidate into next */
    assert(next->start==node.end);
    REGION_REMOVE(regionS_tree_s, &regiontreeS, next);
    REGION_REMOVE(regionL_tree_s, &regiontreeL, next);
    r=next;
    r->start=node.start;
  }
  if(ReleasePages(r ? r->start : node.start, r ? (size_t)r->end - (size_t)r->start : (size_t)node.end - (size_t)node.start))
  {
    if(prev)
    {
      assert(r==prev);
      REGION_REMOVE(regionS_tree_s, &regiontreeS, prev);
    }
    else if(next)
    {
      assert(r==next);
      REGION_REMOVE(regionE_tree_s, &regiontreeE, next);
    }
    if(r) FreeRegionNode(r);
  }
  else
  {
    if(!r)
    {
      r=AllocateRegionNode();
      assert(r);
      if(r)
      {
        r->start=node.start;
        r->end=node.end;
      }
      else
        return -1;
    }
    if(r)
    {
      if(r!=prev)
      {
        REGION_INSERT(regionS_tree_s, &regiontreeS, r);
      }
      if(r!=next)
      {
        REGION_INSERT(regionE_tree_s, &regiontreeE, r);
      }
      REGION_INSERT(regionL_tree_s, &regiontreeL, r);
    }
  }
  assert(NEDTRIE_COUNT(&regiontreeS)==NEDTRIE_COUNT(&regiontreeE));
  assert(NEDTRIE_COUNT(&regiontreeS)==NEDTRIE_COUNT(&regiontreeL));
  return 0;
}

static void *userpage_realloc(void *mem, size_t oldsize, size_t newsize, int flags, unsigned flags2)
{
	return MFAIL;
}

#endif
