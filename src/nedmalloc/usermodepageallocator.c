/* A very fast user mode page allocator implementation enabling all sorts of
useful speed improvements for common malloc operations. (C) 2010 Niall Douglas.


Boost Software License - Version 1.0 - August 17th, 2003

Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:

The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.
*/

#ifdef ENABLE_USERMODEPAGEALLOCATOR

/* This is how many free pages relative to used pages to keep around before
returning them to the system. It gets ignored if system free memory is
perceived to be tight. */
#define USERMODEPAGEALLOCATOR_FREEPAGECACHESIZE(usedpages, freepages) ((usedpages)/4)

/* This is how many subsequent free operations must happen since a page was
freed before it will be eligible to be returned to the system. It helps prevent
large amounts of memory getting repeatedly freed and reallocated. It gets ignored
if system free memory is perceived to be tight. */
#define USERMODEPAGEALLOCATOR_FREEPAGECACHEAGE(usedpages, freepages) 256

/* This defines how frequently the system free memory state should be
checked, and it must be a power of two. */
#define USERMODEPAGEALLOCATOR_SYSTEMFREEMEMORYCHECKRATE 64

/* This turns on the storage of free page status in the top bit of the page frame.
As on x86/x64 page frames start from one going upwards sequentially, this ought to
always be safe and it saves a TLB fetch when scanning for pages to fill in. */
#if (defined(__GNUC__) && (defined(__i386__) || defined(__x86_64__))) || (defined(_MSC_VER) && (defined(_M_IX86) || defined(_M_X64)))
#define USERMODEPAGEALLOCATOR_USEUNSAFEQUICKFREEPAGEINDICATOR
#endif

/* This turns on prefetching of the first cache line of free pages when modifying
a page's free status. It ought to make the code go much quicker, but it doesn't on
my machine at least :( */
/*#define USERMODEPAGEALLOCATOR_USEPREFETCHING*/

/* This puts the user mode page allocator into debug config which means that as
much buffering and caching is disabled as possible in order to best test the code. */
#ifdef DEBUG
#define USERMODEPAGEALLOCATOR_DEBUGCONFIG
#endif

/*
#if (defined(__GNUC__) && (defined(__i386__) || defined(__x86_64__)))
#define PREFETCHCACHELINE(addr, rw, locality) __builtin_prefetch((const void *)(addr), (rw), (locality))
#define STRUCTUREALIGNMENT(alignment)         __attribute__ ((aligned(alignment)))
#elif (defined(_MSC_VER) && (defined(_M_IX86) || defined(_M_X64)))
#define PREFETCHCACHELINE(addr, rw, locality) _mm_prefetch((const char *)(addr), (locality))
#define STRUCTUREALIGNMENT(alignment)         __declspec(align(alignment))
#endif
*/

#ifndef PREFETCHCACHELINE
#define PREFETCHCACHELINE(addr, rw, locality)
#endif
#ifndef STRUCTUREALIGNMENT
#define STRUCTUREALIGNMENT(alignment)
#endif

#ifdef USERMODEPAGEALLOCATOR_DEBUGCONFIG
#undef USERMODEPAGEALLOCATOR_FREEPAGECACHESIZE
#define USERMODEPAGEALLOCATOR_FREEPAGECACHESIZE(usedpages, freepages) ((usedpages)/16)
#undef USERMODEPAGEALLOCATOR_FREEPAGECACHEAGE
#define USERMODEPAGEALLOCATOR_FREEPAGECACHEAGE(usedpages, freepages) 0
#undef USERMODEPAGEALLOCATOR_SYSTEMFREEMEMORYCHECKRATE
#define USERMODEPAGEALLOCATOR_SYSTEMFREEMEMORYCHECKRATE 1
#endif

#include "nedtrie.h"

#ifndef PAGE_SIZE
#define PAGE_SIZE 4096
#endif

#define REGION_ENTRY(type)                        NEDTRIE_ENTRY(type)
#define REGION_HEAD(name, type)                   NEDTRIE_HEAD(name, type)
#define REGION_INIT(treevar)                      NEDTRIE_INIT(treevar)
#define REGION_EMPTY(treevar)                     NEDTRIE_EMPTY(treevar)
#define REGION_GENERATE(proto, treetype, nodetype, link, cmpfunct) NEDTRIE_GENERATE(proto, treetype, nodetype, link, cmpfunct, NEDTRIE_NOBBLEZEROS(treetype))
#define REGION_INSERT(treetype, treevar, node)    NEDTRIE_INSERT(treetype, treevar, node)
#define REGION_REMOVE(treetype, treevar, node)    NEDTRIE_REMOVE(treetype, treevar, node)
#define REGION_FIND(treetype, treevar, node)      NEDTRIE_FIND(treetype, treevar, node)
#define REGION_NFIND(treetype, treevar, node)     NEDTRIE_NFIND(treetype, treevar, node)
#define REGION_MAX(treetype, treevar)             NEDTRIE_MAX(treetype, treevar)
#define REGION_MIN(treetype, treevar)             NEDTRIE_MIN(treetype, treevar)
#define REGION_NEXT(treetype, treevar, node)      NEDTRIE_NEXT(treetype, treevar, node)
#define REGION_PREV(treetype, treevar, node)      NEDTRIE_PREV(treetype, treevar, node)
#define REGION_FOREACH(var, treetype, treevar)    NEDTRIE_FOREACH(var, treetype, treevar)
#define REGION_HASNODEHEADER(treevar, node, link) NEDTRIE_HASNODEHEADER(treevar, node, link)

#if defined(__x86_64__) || defined(_M_X64)
#define REGIONNODEALIGNMENT 128
#else
#define REGIONNODEALIGNMENT 64
#endif
typedef struct region_node_s region_node_t;
struct STRUCTUREALIGNMENT(REGIONNODEALIGNMENT) region_node_s {
    REGION_ENTRY(region_node_s) linkA; /* by start addr */
    REGION_ENTRY(region_node_s) linkL; /* by length */
    region_node_t *prev, *next;
    void *start, *end;
#if defined(__x86_64__) || defined(_M_X64)
    char padding[16];
#else
    char padding[8];
#endif
};
typedef struct regionA_tree_s regionA_tree_t;
REGION_HEAD(regionA_tree_s, region_node_s);
typedef struct regionL_tree_s regionL_tree_t;
REGION_HEAD(regionL_tree_s, region_node_s);

static size_t regionkeyA(const region_node_t *RESTRICT r)
{
  return (size_t) r->start;
}
static size_t regionkeyL(const region_node_t *RESTRICT r)
{
  return (size_t) r->end - (size_t) r->start;
}
REGION_GENERATE(static, regionA_tree_s, region_node_s, linkA, regionkeyA);
REGION_GENERATE(static, regionL_tree_s, region_node_s, linkL, regionkeyL);
typedef struct MemorySource_t MemorySource;
static struct MemorySource_t
{
  regionA_tree_t regiontreeA; /* The list of allocated regions, keyed by start addr */
  regionL_tree_t regiontreeL; /* The list of free regions, keyed by length */
  region_node_t *firstregion; /* The first region by order of addition */
  region_node_t *lastregion;  /* The last region by order of addition */
} lower, upper;

typedef struct OSAddressSpaceReservationData_t
{
  void *addr;
  void *data[2];
} OSAddressSpaceReservationData;
#ifndef WIN32
typedef size_t PageFrameType;

/* This function determines whether the host OS allows user mode physical memory
page mapping. */
static int OSDeterminePhysicalPageSupport(void);

/* This function returns a simple true or false if the host OS allows user mode
physical page mapping */
static int OSHavePhysicalPageSupport(void);

/* This function determines whether the host OS is currently short of memory.
The value is LINEAR between 0.0 (no pressure) and 1.0 (terrible pressure). */
static double OSSystemMemoryPressure(void);

/* This function could ask the host OS for address space, or on embedded systems
it could simply parcel out space via moving a pointer. The second two void *
are some arbitrary extra data to be later passed to OSReleaseAddrSpace(). */
static OSAddressSpaceReservationData OSReserveAddrSpace(size_t space);

/* This function returns address space previously allocated using
OSReserveAddrSpace(). It is guaranteed to exactly match what was previously
returned by that function. */
static int OSReleaseAddrSpace(OSAddressSpaceReservationData *data, size_t space);

/* This function obtains physical memory pages, either by asking the host OS
or on embedded systems by simply pulling them from a free page ring list. */
static size_t OSObtainMemoryPages(PageFrameType *buffer, size_t number, OSAddressSpaceReservationData *data);

/* This function returns previously obtained physical memory pages. */
static size_t OSReleaseMemoryPages(PageFrameType *buffer, size_t number, OSAddressSpaceReservationData *data);

/* This function causes the specified set of physical memory pages to be
mapped at the specified address. On an embedded system this would simply
modify the MMU and flush the appropriate TLB entries.
*/
static size_t OSRemapMemoryPagesOntoAddr(void *addr, size_t entries, PageFrameType *pageframes, OSAddressSpaceReservationData *data);

/* This function causes the specified set of physical memory pages to be
mapped at the specified set of addresses. On an embedded system this would
simply modify the MMU and flush the appropriate TLB entries. It works like this:

for(size_t n=0; n<entries; n++, addrs++, pageframes++) {
  if(*pageframe)
    Map(*addr, *pageframe);
  else
    Unmap(*addr);
}
*/
static size_t OSRemapMemoryPagesOntoAddrs(void **addrs, size_t entries, PageFrameType *pageframes, OSAddressSpaceReservationData *data);
#else
static enum {
  DISABLEEVERYTHING=1,
  NOPHYSICALPAGESUPPORT=2,
  HAVEPHYSICALPAGESUPPORT=4
} PhysicalPageSupport;

#ifdef ENABLE_PHYSICALPAGEEMULATION
/* Windows has the curious problem of using 4Kb pages but requiring those pages
to be mapped at 64Kb aligned address. By far the easiest solution is to pretend
that we actually have 64Kb pages. */
#undef PAGE_SIZE
#define PAGE_SIZE 65536
typedef struct PageFrameType_t
{
  ULONG_PTR pages[16];
} PageFrameType;
#else
typedef ULONG_PTR PageFrameType;
#endif

/* Returns 1 for bad compile, 2 for no support on this machine/user,
4 for supported */
#pragma comment(lib, "advapi32.lib")
static int OSDeterminePhysicalPageSupport(void)
{
  if(!PhysicalPageSupport)
  { /* Quick test */
    PageFrameType pageframe;
    size_t no=sizeof(PageFrameType)/sizeof(ULONG_PTR);
    SYSTEM_INFO si={0};
		{
			HANDLE token;
			if(OpenProcessToken(GetCurrentProcess(), TOKEN_ADJUST_PRIVILEGES, &token))
			{
				TOKEN_PRIVILEGES privs={1};
				if(LookupPrivilegeValue(NULL, SE_LOCK_MEMORY_NAME, &privs.Privileges[0].Luid))
				{
					privs.Privileges[0].Attributes=SE_PRIVILEGE_ENABLED;
					if(!AdjustTokenPrivileges(token, FALSE, &privs, 0, NULL, NULL) || GetLastError()!=S_OK)
					{
					}
				}
				CloseHandle(token);
			}
		}
    if(AllocateUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &no, (PULONG_PTR) &pageframe))
    {
      FreeUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &no, (PULONG_PTR) &pageframe);
      PhysicalPageSupport=HAVEPHYSICALPAGESUPPORT;
    }
    else
    {
      PhysicalPageSupport=NOPHYSICALPAGESUPPORT;
			fprintf(stderr, "User Mode Page Allocator: Failed to allocate physical memory pages (does the user running this process have the right to lock pages in memory?). User Mode Page Allocator will not be used.\n");
      OutputDebugStringA("User Mode Page Allocator: Failed to allocate physical memory pages (does the user running this process have the right to lock pages in memory?). User Mode Page Allocator will not be used.\n");
    }
    GetSystemInfo(&si);
#ifdef ENABLE_PHYSICALPAGEEMULATION
    if(si.dwAllocationGranularity!=PAGE_SIZE)
    {
      assert(si.dwAllocationGranularity==PAGE_SIZE);
      fprintf(stderr, "User Mode Page Allocator: Allocation granularity is %u not %u. Please recompile with corrected PAGE_SIZE\n", si.dwAllocationGranularity, PAGE_SIZE);
      PhysicalPageSupport=DISABLEEVERYTHING;
    }
    if(si.dwAllocationGranularity/si.dwPageSize!=sizeof(PageFrameType)/sizeof(ULONG_PTR))
    {
      assert(si.dwAllocationGranularity/si.dwPageSize==sizeof(PageFrameType)/sizeof(ULONG_PTR));
      fprintf(stderr, "User Mode Page Allocator: Pages per PageFrameType is %u not %u. Please recompile with corrected PageFrameType definition\n", si.dwAllocationGranularity/si.dwPageSize, sizeof(PageFrameType)/sizeof(ULONG_PTR));
      PhysicalPageSupport=DISABLEEVERYTHING;
    }
#else
    if(si.dwPageSize!=PAGE_SIZE)
    {
      assert(si.dwPageSize==PAGE_SIZE);
      fprintf(stderr, "User Mode Page Allocator: Page size is %u not %u. Please recompile with corrected PAGE_SIZE\n", si.dwPageSize, PAGE_SIZE);
      PhysicalPageSupport=DISABLEEVERYTHING;
    }
#endif
  }
  return PhysicalPageSupport;
}
static int OSHavePhysicalPageSupport(void)
{
  if(!PhysicalPageSupport) OSDeterminePhysicalPageSupport();
  return HAVEPHYSICALPAGESUPPORT==PhysicalPageSupport;
}
static double OSSystemMemoryPressure(void)
{
  MEMORYSTATUSEX ms={sizeof(MEMORYSTATUSEX)};
  if(!GlobalMemoryStatusEx(&ms))
    return 0;
  return ms.dwMemoryLoad/100.0;
}
static OSAddressSpaceReservationData OSReserveAddrSpace(size_t space)
{
  OSAddressSpaceReservationData ret={0};
  if(!PhysicalPageSupport) OSDeterminePhysicalPageSupport();
  if(DISABLEEVERYTHING==PhysicalPageSupport) return ret;
  if(HAVEPHYSICALPAGESUPPORT==PhysicalPageSupport)
  {
    ret.addr=VirtualAlloc(NULL, space, MEM_RESERVE|MEM_PHYSICAL, PAGE_READWRITE);
  }
#ifdef ENABLE_PHYSICALPAGEEMULATION
  if(!ret.addr)
  {
    HANDLE fmh;
    fmh = CreateFileMapping(INVALID_HANDLE_VALUE, NULL, PAGE_READWRITE|SEC_RESERVE,
#if defined(_M_IA64) || defined(_M_X64) || defined(WIN64)
                            (DWORD)(space>>32),
#else
                            0,
#endif
                            (DWORD)(space&((DWORD)-1)), NULL);
    if(fmh)
    { /* This is breathtakingly inefficient, but win32 leaves us no choice :(
         At least this function is called very infrequently. */
      while((ret.addr=VirtualAlloc(NULL, space, MEM_RESERVE, PAGE_READWRITE)))
      {
        void *RESTRICT seg;
        VirtualFree(ret.addr, 0, MEM_RELEASE);
        for(seg=ret.addr; seg<(void*)((size_t) ret.addr + space); seg=(void *)((size_t) seg + Win32granularity))
        {
          if(!VirtualAlloc(seg, Win32granularity, MEM_RESERVE, PAGE_READWRITE))
            break;
        }
        if(seg==(void*)((size_t) ret.addr + space))
          break;
        else
        {
          for(; seg>=ret.addr; seg=(void *)((size_t) seg - Win32granularity))
            VirtualFree(seg, 0, MEM_RELEASE);
        }
      }
      if(!ret.addr)
        CloseHandle(fmh);
      else
      {
        ret.data[0]=(void *) fmh;
        ret.data[1]=(void *)(size_t) 1;
      }
    }
  }
#endif
  return ret;
}
static int OSReleaseAddrSpace(OSAddressSpaceReservationData *RESTRICT data, size_t space)
{
  if(!data->data[0])
    return VirtualFree(data->addr, 0, MEM_RELEASE);
#ifdef ENABLE_PHYSICALPAGEEMULATION
  else
  {
    void *seg;
    CloseHandle((HANDLE)data->data[0]);
    for(seg=data->addr; seg<(void*)((size_t) data->addr + space); seg=(void *)((size_t) seg + Win32granularity))
      VirtualFree(seg, 0, MEM_RELEASE);
    return 1;
  }
#endif
  return 0;
}
static size_t OSObtainMemoryPages(PageFrameType *RESTRICT buffer, size_t number, OSAddressSpaceReservationData *RESTRICT data)
{
  if(!data->data[0])
  {
#ifdef ENABLE_PHYSICALPAGEEMULATION
    number*=sizeof(PageFrameType)/sizeof(ULONG_PTR);
#endif
#if 1
    if(!AllocateUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &number, (PULONG_PTR) buffer))
    {
      if(number)
        FreeUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &number, (PULONG_PTR) buffer);
      return 0;
    }
#else
    {
      size_t n;
      PageFrameType *RESTRICT bptr;
      for(n=0, bptr=buffer; n<number; n++, bptr++)
      {
        size_t no=1;
        if(!AllocateUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &no, (PULONG_PTR) bptr))
        {
          for(bptr--; bptr>=buffer; bptr--)
          {
            no=1;
            FreeUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &no, (PULONG_PTR) bptr);
          }
          return 0;
        }
      }
    }
#endif
#ifdef ENABLE_PHYSICALPAGEEMULATION
    number/=sizeof(PageFrameType)/sizeof(ULONG_PTR);
#endif
    return number;
  }
#ifdef ENABLE_PHYSICALPAGEEMULATION
  else
  {
    size_t n;
    ULONG_PTR *RESTRICT pf=(ULONG_PTR *RESTRICT) &data->data[1];
    for(n=0; n<number*(sizeof(PageFrameType)/sizeof(ULONG_PTR)); n++)
      ((ULONG_PTR *) buffer)[n]=(*pf)++;
    return number;
  }
#endif
  return 0;
}
static size_t OSReleaseMemoryPages(PageFrameType *RESTRICT buffer, size_t number, OSAddressSpaceReservationData *RESTRICT data)
{
  if(!data->data[0])
  {
#ifdef ENABLE_PHYSICALPAGEEMULATION
    number*=sizeof(PageFrameType)/sizeof(ULONG_PTR);
#endif
#if 1
    if(!FreeUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &number, (PULONG_PTR) buffer)) return 0;
#else
    {
      size_t n;
      PageFrameType *RESTRICT bptr;
      for(n=0, bptr=buffer; n<number; n++, bptr++)
      {
        size_t no=1;
        if(!FreeUserPhysicalPages((HANDLE)(size_t)-1, (PULONG_PTR) &no, (PULONG_PTR) bptr))
        {
          assert(0);
        }
      }
    }
#endif
#ifdef ENABLE_PHYSICALPAGEEMULATION
    number/=sizeof(PageFrameType)/sizeof(ULONG_PTR);
#endif
#ifdef DEBUG
    /*for(n=0; n<number*(sizeof(PageFrameType)/sizeof(ULONG_PTR)); n++)
      ((ULONG_PTR *) buffer)[n]=0;*/
#endif
    return number;
  }
  /* Always fail if we are emulating physical pages */
  return 0;
}
static 
__declspec(noinline)
size_t OSRemapMemoryPagesOntoAddr(void *addr, size_t entries, PageFrameType *RESTRICT pageframes, OSAddressSpaceReservationData *RESTRICT data)
{
  if(!data->data[0])
  {
    BOOL ret;
#ifdef ENABLE_PHYSICALPAGEEMULATION
    entries*=sizeof(PageFrameType)/sizeof(ULONG_PTR);
#endif
#if 1
    ret=MapUserPhysicalPages(addr, entries, (PULONG_PTR) pageframes);
    if(!ret)
    {
      assert(ret);
    }
    return ret;
#else
    {
      size_t n;
      PageFrameType *RESTRICT bptr;
      for(n=0, bptr=pageframes; n<entries; n++, bptr++, addr=(void *)((size_t) addr + PAGE_SIZE))
      {
        ret=MapUserPhysicalPages(addr, 1, pageframes ? bptr : NULL);
        if(!ret)
        {
          assert(ret);
          return 0;
        }
      }
      return 1;
    }
#endif
  }
#ifdef ENABLE_PHYSICALPAGEEMULATION
  else
  {
    size_t n, ret=1;
    PageFrameType *RESTRICT pfa, *RESTRICT pf;
    for(n=0; n<entries; n++, addr=(void *)((size_t) addr + PAGE_SIZE), pageframes++)
    {
      if(*pageframe)
      {
        size_t filemappingoffset=PAGE_SIZE*((*pageframe)-1);
        /* Change reservation for next segment */
        if(!VirtualFree(addr, 0, MEM_RELEASE)) ret=0;
        if(!MapViewOfFileEx((HANDLE) data.data[0], FILE_MAP_ALL_ACCESS,
#if defined(_M_IA64) || defined(_M_X64) || defined(WIN64)
                           (DWORD)(filemappingoffset>>32),
#else
                           0,
#endif
                           (DWORD)(filemappingoffset & (DWORD)-1), PAGE_SIZE, addr)) ret=0;
      }
      else
      {
        if(!UnmapViewOfFile(addr)) ret=0;
        /* Rereserve */
        if(!VirtualAlloc(addr, PAGE_SIZE, MEM_RESERVE, PAGE_READWRITE)) ret=0;
      }
    }
    return ret;
  }
#endif
  return 0;
}
static
__declspec(noinline)
size_t OSRemapMemoryPagesOntoAddrs(void *RESTRICT *addrs, size_t entries, PageFrameType *RESTRICT pageframes, OSAddressSpaceReservationData *RESTRICT data)
{
#ifdef DEBUG
  size_t n;
  void *RESTRICT *addr;
  PageFrameType *RESTRICT pf;
  assert(entries);
  for(addr=addrs, pf=pageframes, n=0; n<entries; n++, addr++, pf++)
  {
    assert(*addr);
  }
#endif
  if(!data->data[0])
  {
    BOOL ret;
#ifdef ENABLE_PHYSICALPAGEEMULATION
    entries*=sizeof(PageFrameType)/sizeof(ULONG_PTR);
#endif
#if 1
    ret=MapUserPhysicalPagesScatter(addrs, entries, (PULONG_PTR) pageframes);
    if(!ret)
    {
      assert(ret);
    }
#else
    {
      size_t n;
      void **aptr;
      PageFrameType *RESTRICT bptr;
      for(n=0, aptr=addrs, bptr=pageframes; n<entries; n++, aptr++, bptr++)
      {
        ret=MapUserPhysicalPages(*aptr, 1, *bptr ? bptr : NULL);
        if(!ret)
        {
          assert(ret);
          return 0;
        }
      }
      return 1;
    }
#endif
    return ret;
  }
#ifdef ENABLE_PHYSICALPAGEEMULATION
  else
  {
    size_t n, ret=1;
    PageFrameType *RESTRICT pfa, *RESTRICT pf;
    for(n=0; n<entries; n++, addrs++, pageframes++)
    {
      if(*pageframe)
      {
        size_t filemappingoffset=PAGE_SIZE*((*pageframe)-1);
        /* Change reservation for next segment */
        if(!VirtualFree(*addrs, 0, MEM_RELEASE)) ret=0;
        if(!MapViewOfFileEx((HANDLE) data.data[0], FILE_MAP_ALL_ACCESS,
#if defined(_M_IA64) || defined(_M_X64) || defined(WIN64)
                           (DWORD)(filemappingoffset>>32),
#else
                           0,
#endif
                           (DWORD)(filemappingoffset & (DWORD)-1), PAGE_SIZE, *addrs)) ret=0;
      }
      else
      {
        if(!UnmapViewOfFile(*addrs)) ret=0;
        /* Rereserve */
        if(!VirtualAlloc(*addrs, PAGE_SIZE, MEM_RESERVE, PAGE_READWRITE)) ret=0;
      }
    }
    return ret;
  }
#endif
  return 0;
}

#endif

#ifdef USERMODEPAGEALLOCATOR_USEUNSAFEQUICKFREEPAGEINDICATOR
#define USERMODEPAGEALLOCATOR_ISPAGEFREE(pageframe, freepage) ((pageframe) & ((size_t)1<<(8*sizeof(size_t)-1)))
#define USERMODEPAGEALLOCATOR_SETPAGEFREEBIT(pageframe)       ((pageframe) | ((size_t)1<<(8*sizeof(size_t)-1)))
#define USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(pageframe)   ((pageframe) & ~((size_t)1<<(8*sizeof(size_t)-1)))
#else
#define USERMODEPAGEALLOCATOR_ISPAGEFREE(pageframe, freepage) (FREEPAGEHEADERMAGIC==(freepage)->magic)
#define USERMODEPAGEALLOCATOR_SETPAGEFREEBIT(pageframe)       (pageframe)
#define USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(pageframe)   (pageframe)
#endif

/* Maps an address reservation */
#define FREEPAGEHEADERMAGIC (*(size_t *)"UMPAFREE")
typedef struct FreePageHeader_t FreePageHeader;
struct STRUCTUREALIGNMENT(4096) FreePageHeader_t
{
  size_t magic;
  FreePageHeader *older, *newer;
  size_t age;
  size_t dirty;
};
static void ClearFreePageHeader(FreePageHeader *RESTRICT fph)
{
#if 1
  memset(fph, 0, sizeof(FreePageHeader));
#else
  fph->magic=0;
  fph->older=fph->newer=0;
  fph->age=fph->dirty=0;
#endif
}
typedef struct AddressSpaceReservation_s AddressSpaceReservation_t;
static struct AddressSpaceReservation_s
{
  OSAddressSpaceReservationData OSreservedata;
  AddressSpaceReservation_t *RESTRICT next;
  void *front, *frontptr;         /* Grows upward */
  void *back, *backptr;           /* Grows downward */
  size_t opcount;
  FreePageHeader *oldestclean, *newestclean;
  FreePageHeader *oldestdirty, *newestdirty;
  size_t freepages;
  size_t usedpages;               /* Doesn't include pages used to store this structure */
  PageFrameType pagemapping[1];   /* Includes this structure */
} *RESTRICT addressspacereservation;

static void ValidateFreePageLists(AddressSpaceReservation_t *RESTRICT addr)
{
#ifndef NDEBUG
#if 0
  FreePageHeader *RESTRICT freepage;
  size_t count=0;
  for(freepage=addr->oldestclean; freepage; freepage=freepage->newer)
  {
    size_t freepagepfidx=((size_t)freepage-(size_t)addr)/PAGE_SIZE;
    assert(addr->pagemapping[freepagepfidx]);
    assert(FREEPAGEHEADERMAGIC==freepage->magic);
    assert((!freepage->older && addr->oldestclean==freepage) || freepage->older->newer==freepage);
    assert((!freepage->newer && addr->newestclean==freepage) || freepage->newer->older==freepage);
    assert(freepage->age<=addr->opcount);
    assert(!freepage->dirty);
    count++;
  }
  for(freepage=addr->oldestdirty; freepage; freepage=freepage->newer)
  {
    size_t freepagepfidx=((size_t)freepage-(size_t)addr)/PAGE_SIZE;
    assert(addr->pagemapping[freepagepfidx]);
    assert(FREEPAGEHEADERMAGIC==freepage->magic);
    assert((!freepage->older && addr->oldestdirty==freepage) || freepage->older->newer==freepage);
    assert((!freepage->newer && addr->newestdirty==freepage) || freepage->newer->older==freepage);
    assert(freepage->age<=addr->opcount);
    assert(freepage->dirty);
    count++;
  }
  assert(count==addr->freepages);
#endif
#endif
}
static void ValidatePageMappings(AddressSpaceReservation_t *RESTRICT addr)
{
#ifndef NDEBUG
#if 0
#ifdef _MSC_VER
  PageFrameType *RESTRICT pf;
  size_t n;
  for(pf=addr->pagemapping, n=0; n<((size_t) addr->frontptr + 16*PAGE_SIZE - (size_t) addr); pf++, n+=PAGE_SIZE)
  {
    volatile size_t *RESTRICT pageaddr=(size_t *RESTRICT)((size_t) addr + n + 0x300/* For some odd reason Windows maps 0x2F0 extra bytes */);
    int faulted=0;
    if(*pf && n)
    { /* Verify that this is indeed a valid page frame */
      PageFrameType t=USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(*pf);
      assert(OSRemapMemoryPagesOntoAddr((void *)((size_t) addr + n), 1, NULL, &addr->OSreservedata));
      assert(OSRemapMemoryPagesOntoAddr((void *)((size_t) addr + n), 1, &t, &addr->OSreservedata));
    }
#if 1
    {
      char buffer[8];
      faulted=!ReadProcessMemory((HANDLE)(size_t)-1, (void *) pageaddr, buffer, 1, NULL);
    }
#else
    __try
    {
      *pageaddr;
    }
    __except(1)
    {
      faulted=1;
    }
#endif
    assert(faulted==!*pf);
  }
#endif
#endif
#endif
}
static AddressSpaceReservation_t *ReserveSpace(size_t space)
{
  const size_t RESERVEALWAYSLEAVEFREE=64*1024*1024; /* Windows goes seriously screwy if you take away all address space */
  OSAddressSpaceReservationData addrR={0};
  AddressSpaceReservation_t *RESTRICT addr=0;
  size_t pagemappingsize, n, pagesallocated;
  PageFrameType pagebuffer[256];
  if(space<(size_t)1<<30 /* 1Gb */)
  {
    space=(size_t)1<<30;
    if(8==sizeof(size_t)) space<<=2; /* Go for 4Gb chunks on 64 bit */
  }
  while(space>=RESERVEALWAYSLEAVEFREE && !(addrR=OSReserveAddrSpace(space)).addr)
    space>>=1;
  if(space<RESERVEALWAYSLEAVEFREE) return 0;
  pagemappingsize=sizeof(AddressSpaceReservation_t)+sizeof(PageFrameType)*((space/PAGE_SIZE)-2);
  pagemappingsize=(pagemappingsize+PAGE_SIZE-1) &~(PAGE_SIZE-1);
  pagemappingsize/=PAGE_SIZE;
  /* We now need pagemappingsize number of pages in order to store the mapping tables, but
  because this could be as much as 4Mb of stuff we'll need to do it in chunks to avoid
  breaking the stack. */
  for(n=0; n<pagemappingsize; n+=pagesallocated)
  {
    size_t torequest=sizeof(pagebuffer)/sizeof(PageFrameType);
    void *mapaddr=(void *)((size_t) addrR.addr + n*PAGE_SIZE);
    if(torequest>pagemappingsize-n) torequest=pagemappingsize-n;
    if(!(pagesallocated=OSObtainMemoryPages(pagebuffer, torequest, &addrR)))
      goto badexit;
    if(!OSRemapMemoryPagesOntoAddr(mapaddr, pagesallocated, pagebuffer, &addrR))
      goto badexit;
    if(!n)
    { /* This is the first run, so install AddressSpaceReservation */
      addr=(AddressSpaceReservation_t *RESTRICT) addrR.addr;
      addr->OSreservedata=addrR;
      addr->front=addr->frontptr=(void *)((size_t)addr+pagemappingsize*PAGE_SIZE);
      addr->back=addr->backptr=(void *)((size_t)addr+space);
    }
    /* Add these new pages to the page mappings. Because we are premapping in new pages,
    we are guaranteed to have memory already there ready for us. */
    for(torequest=0; torequest<pagesallocated; torequest++)
      addr->pagemapping[n+torequest]=pagebuffer[torequest];
  }
  ValidatePageMappings(addr);
  return addr;
badexit:
  /* Firstly throw away any just allocated pages */
  if(pagesallocated)
    OSReleaseMemoryPages(pagebuffer, pagesallocated, &addrR);
  if(addr)
  { /* Now throw away any previously stored */
    OSReleaseMemoryPages(addr->pagemapping, n, &addrR);
  }
  OSReleaseAddrSpace(&addrR, space);
  return 0;
}
static int CheckFreeAddressSpaces(AddressSpaceReservation_t *RESTRICT *RESTRICT _addr)
{
  AddressSpaceReservation_t *RESTRICT addr=*_addr;
  if(!addr->next || CheckFreeAddressSpaces(&addr->next))
  {
    assert(!addr->next);
    if(0==addr->usedpages)
    {
      size_t size=(size_t)addr->back-(size_t)addr;
      assert(addr->frontptr==addr->front);
      assert(addr->backptr==addr->back);
      if(OSReleaseAddrSpace(&addr->OSreservedata, size))
      {
        *_addr=0;
        return 1;
      }
    }
  }
  return 0;
}
#ifdef USERMODEPAGEALLOCATOR_DEBUGCONFIG
#define REMAPMEMORYPAGESBLOCKSIZE 16
#else
#define REMAPMEMORYPAGESBLOCKSIZE 1024
#endif
typedef struct STRUCTUREALIGNMENT(16) RemapMemoryPagesBlock_t
{
  void *addrs[REMAPMEMORYPAGESBLOCKSIZE];
  PageFrameType pageframes[REMAPMEMORYPAGESBLOCKSIZE];
  size_t idx;
} RemapMemoryPagesBlock;
static size_t FillWithFreePages(AddressSpaceReservation_t *RESTRICT addr, RemapMemoryPagesBlock *RESTRICT memtodecommit, RemapMemoryPagesBlock *RESTRICT memtocommit, void *freespaceaddr, PageFrameType *RESTRICT start, PageFrameType *RESTRICT end, int needclean)
{
  size_t n, pages=end-start;
  PageFrameType *RESTRICT pf=start;
  for(n=0; n<pages && addr->freepages; n++, pf++, freespaceaddr=(void *)((size_t) freespaceaddr + PAGE_SIZE))
  {
    FreePageHeader *RESTRICT freepage, *RESTRICT *RESTRICT freepageaddr=0, *RESTRICT *RESTRICT freepagenaddr=0;
    PageFrameType freepageframe;
    size_t freepagepfidx;
    assert(!*pf);
    if(needclean && addr->oldestclean)
    {
      assert(!addr->oldestclean->older);
      freepage=addr->oldestclean;
      freepageaddr=&addr->oldestclean;
      freepagenaddr=&addr->newestclean;
    }
    else if(addr->oldestdirty)
    {
      assert(!addr->oldestdirty->older);
      freepage=addr->oldestdirty;
      freepageaddr=&addr->oldestdirty;
      freepagenaddr=&addr->newestdirty;
    }
    else if(!needclean && addr->oldestclean)
    {
      assert(!addr->oldestclean->older);
      freepage=addr->oldestclean;
      freepageaddr=&addr->oldestclean;
      freepagenaddr=&addr->newestclean;
    }
    /* Add to the list of pages to demap */
    memtodecommit->addrs[memtodecommit->idx]=freepage;
    /*memtodecommit->pageframes[memtodecommit->idx]=0;*/
    memtodecommit->idx++;
    /* Add to the list of pages to remap */
    freepagepfidx=((size_t)freepage-(size_t)addr)/PAGE_SIZE;
    freepageframe=addr->pagemapping[freepagepfidx];
    addr->pagemapping[freepagepfidx]=0;
    memtocommit->addrs[memtocommit->idx]=freespaceaddr;
    *pf=USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(freepageframe);
    memtocommit->pageframes[memtocommit->idx]=USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(freepageframe);
    memtocommit->idx++;
    /* Remove from free page lists */
    assert(FREEPAGEHEADERMAGIC==freepage->magic);
    assert(!freepage->older);
    *freepageaddr=freepage->newer;
    if(*freepageaddr)
      (*freepageaddr)->older=0;
    else
      *freepagenaddr=0;
    addr->freepages--;
    addr->usedpages++;
#if MMAP_CLEARS
    if(needclean && freepage->dirty)
      memset(freepage, 0, PAGE_SIZE);
    else
#endif
      ClearFreePageHeader(freepage);
    ValidateFreePageLists(addr);

    if(REMAPMEMORYPAGESBLOCKSIZE==memtocommit->idx)
    {
      if(memtodecommit->idx) OSRemapMemoryPagesOntoAddrs(memtodecommit->addrs, memtodecommit->idx, NULL, &addr->OSreservedata);
      OSRemapMemoryPagesOntoAddrs(memtocommit->addrs, memtocommit->idx, memtocommit->pageframes, &addr->OSreservedata);
      memtodecommit->idx=memtocommit->idx=0;
      ValidatePageMappings(addr);
    }
  }
  /* Allocate more pages if needed */
  while(pages-n>0)
  {
    size_t newpagesnow=pages-n, newpagesobtained, m;
    if(newpagesnow>REMAPMEMORYPAGESBLOCKSIZE-memtocommit->idx) newpagesnow=REMAPMEMORYPAGESBLOCKSIZE-memtocommit->idx;
    newpagesobtained=OSObtainMemoryPages(&memtocommit->pageframes[memtocommit->idx], newpagesnow, &addr->OSreservedata);
    if(newpagesnow!=newpagesobtained)
    {
      if(newpagesobtained) OSReleaseMemoryPages(&memtocommit->pageframes[memtocommit->idx], newpagesobtained, &addr->OSreservedata);
      return n;
    }
    for(m=memtocommit->idx; m<memtocommit->idx+newpagesobtained; m++, pf++, freespaceaddr=(void *)((size_t) freespaceaddr + PAGE_SIZE))
    {
      memtocommit->addrs[m]=freespaceaddr;
      *pf=memtocommit->pageframes[m];
    }
    memtocommit->idx+=newpagesobtained;
    n+=newpagesobtained;
    if(REMAPMEMORYPAGESBLOCKSIZE==memtocommit->idx)
    {
      if(memtodecommit->idx) OSRemapMemoryPagesOntoAddrs(memtodecommit->addrs, memtodecommit->idx, NULL, &addr->OSreservedata);
      OSRemapMemoryPagesOntoAddrs(memtocommit->addrs, memtocommit->idx, memtocommit->pageframes, &addr->OSreservedata);
      memtodecommit->idx=memtocommit->idx=0;
      ValidatePageMappings(addr);
    }
  }
  return n;
}
static int DetachFreePage(AddressSpaceReservation_t *RESTRICT addr, PageFrameType *RESTRICT pagemappings, FreePageHeader *RESTRICT freepage)
{
  int wipeall=0;
  FreePageHeader *RESTRICT *RESTRICT prevnextaddr=0, *RESTRICT *RESTRICT nextprevaddr=0;
  *pagemappings=USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(*pagemappings);
  if(freepage->older)
  {
    assert(freepage->older->newer==freepage);
    prevnextaddr=&freepage->older->newer;
  }
  else if(addr->oldestdirty==freepage)
  {
    assert(!freepage->older);
    prevnextaddr=&addr->oldestdirty;
    wipeall=1;
  }
  else if(addr->oldestclean==freepage)
  {
    assert(!freepage->older);
    prevnextaddr=&addr->oldestclean;
  }
  if(freepage->newer)
  {
    assert(freepage->newer->older==freepage);
    nextprevaddr=&freepage->newer->older;
  }
  else if(addr->newestdirty==freepage)
  {
    assert(!freepage->newer);
    nextprevaddr=&addr->newestdirty;
    wipeall=1;
  }
  else if(addr->newestclean==freepage)
  {
    assert(!freepage->newer);
    nextprevaddr=&addr->newestclean;
  }
  if(!wipeall) wipeall=(int)freepage->dirty;
  *prevnextaddr=freepage->newer;
  *nextprevaddr=freepage->older;
  return wipeall;
}
static int ReleasePages(void *mem, size_t size, int dontfreeVA);
static void *AllocatePages(void *mem, size_t size, unsigned flags)
{
  AddressSpaceReservation_t *RESTRICT addr;
  if(!addressspacereservation && !(addressspacereservation=ReserveSpace(0)))
  {
    fprintf(stderr, "User Mode Page Allocator: Failed to allocate initial address space\n");
    abort();
  }
  for(addr=addressspacereservation; addr; addr=addr->next)
  {
    int fromback=(flags & USERPAGE_TOPDOWN);
    if((mem && ((mem>=addr->front && mem<addr->frontptr && !(fromback=0)) || (mem>=addr->backptr && mem<addr->back && (fromback=1)))
      || (!mem && (size_t) addr->backptr - (size_t) addr->frontptr>=size)))
    {
      RemapMemoryPagesBlock memtodecommit, memtocommit;
      size_t n, sizeinpages=size/PAGE_SIZE;
      void *ret=mem ? mem : ((fromback) ? (void *)((size_t) addr->backptr - size) : addr->frontptr), *retptr;
      PageFrameType *RESTRICT pagemappingsbase=addr->pagemapping+((size_t)ret-(size_t)addr)/PAGE_SIZE, *RESTRICT pagemappings;
      int needtofillwithfree=0;
      memtodecommit.idx=memtocommit.idx=0;
      if(!mem)
      {
        if(fromback)
          addr->backptr=(void *)((size_t) addr->backptr - size);
        else
          addr->frontptr=(void *)((size_t) addr->frontptr + size);
      }
      if(!(flags & USERPAGE_NOCOMMIT))
      { /* We leave memory still held by the application mapped at the addresses it was mapped at
        when freed and only nobble these when we need new pages. Hence between addresses ret and
        ret+size there may be a patchwork of already allocated regions, so what we do is to firstly
        delink any already mapped pages from the free page list and then to batch the filling in of
        the blank spots sixteen at a time. */
        pagemappings=pagemappingsbase;
        retptr=ret;
#ifdef USERMODEPAGEALLOCATOR_USEPREFETCHING
        { /* Prefetch these - it makes a huge difference to speed! */
          PageFrameType *RESTRICT pagemappings2=pagemappings;
          FreePageHeader *RESTRICT freepage2=(FreePageHeader *RESTRICT) retptr;
          for(n=0; n<sizeinpages; n++, pagemappings2++, freepage2=(FreePageHeader *RESTRICT)((size_t)freepage2 + PAGE_SIZE))
          {
            if(*pagemappings && USERMODEPAGEALLOCATOR_ISPAGEFREE(*pagemappings, freepage2))
            {
              PREFETCHCACHELINE(freepage2, 1, 3);
            }
          }
        }
#endif
        for(n=0; n<sizeinpages; n++, pagemappings++, retptr=(void *)((size_t)retptr + PAGE_SIZE))
        {
          if(*pagemappings)
          {
            FreePageHeader *RESTRICT freepage=(FreePageHeader *RESTRICT) retptr;
            if(USERMODEPAGEALLOCATOR_ISPAGEFREE(*pagemappings, freepage))
            {
              int wipeall=DetachFreePage(addr, pagemappings, freepage);
              addr->freepages--;
              addr->usedpages++;
#if MMAP_CLEARS
              if(fromback && wipeall)
                memset(freepage, 0, PAGE_SIZE);
              else
#endif
                ClearFreePageHeader(freepage);
              ValidateFreePageLists(addr);
            }
          }
          else
            needtofillwithfree=1;
        }
        if(needtofillwithfree)
        {
          pagemappings=pagemappingsbase;
          retptr=ret;
          for(n=0; n<sizeinpages; n++, pagemappings++, retptr=(void *)((size_t)retptr + PAGE_SIZE))
          {
            if(!*pagemappings)
            {
              void *emptyaddrstart=retptr;
              PageFrameType *emptyframestart=pagemappings;
              size_t filled;
              for(; n<sizeinpages && !*pagemappings; n++, pagemappings++, retptr=(void *)((size_t)retptr + PAGE_SIZE));
              if(pagemappings-emptyframestart!=(filled=FillWithFreePages(addr, &memtodecommit, &memtocommit, emptyaddrstart, emptyframestart, pagemappings, fromback)))
              { /* We failed to allocate everything, so release */
                assert(0);
                ReleasePages(ret, size, 0);
                return 0;
              }
              if(n==sizeinpages) break;
            }
          }
          if(memtocommit.idx)
          {
            if(memtodecommit.idx) OSRemapMemoryPagesOntoAddrs(memtodecommit.addrs, memtodecommit.idx, NULL, &addr->OSreservedata);
            OSRemapMemoryPagesOntoAddrs(memtocommit.addrs, memtocommit.idx, memtocommit.pageframes, &addr->OSreservedata);
            memtodecommit.idx=memtocommit.idx=0;
            ValidatePageMappings(addr);
          }
        }
      }
      return ret;
    }
    if(!addr->next)
    { /* This is just to catch a bug which tripped me up once and might happen again */
      if(mem<(void *) 0x40000) abort();
      addr->next=ReserveSpace((size_t) 1<<(nedtriebitscanr(size-1)+1));
    }
  }
  return 0;
}
static AddressSpaceReservation_t *RESTRICT AddressSpaceFromMem(int *RESTRICT fromback, void *mem)
{
  AddressSpaceReservation_t *RESTRICT addr;
  for(addr=addressspacereservation; addr; addr=addr->next)
  {
    if(mem>=addr->front && mem<addr->back)
    {
      if(fromback) *fromback=mem>=addr->backptr;
      return addr;
    }
  }
  return 0;
}
static double mypow8(double v)
{
  double t1=v*v;
  double t2=t1*t1;
  return t2+t2;
}
static int ReleasePages(void *mem, size_t size, int dontfreeVA)
{ /* Returns 1 if address space was freed */
  int fromback;
  AddressSpaceReservation_t *RESTRICT addr=AddressSpaceFromMem(&fromback, mem);
  if(addr)
  {
    FreePageHeader *RESTRICT freepage=(FreePageHeader *RESTRICT) mem;
    size_t n, sizeinpages=size/PAGE_SIZE;
    PageFrameType *RESTRICT pagemappings=addr->pagemapping+((size_t)freepage-(size_t)addr)/PAGE_SIZE;
    int dofreesystemmemorycheck=0;
    if(mem>=addr->frontptr && mem<addr->backptr)
    {
      fprintf(stderr, "User Mode Page Allocator: Attempt to free memory in dead man's land\n");
      assert(0);
      abort();
    }
#ifdef USERMODEPAGEALLOCATOR_USEPREFETCHING
    { /* Prefetch these - it makes a huge difference to speed! */
      PageFrameType *RESTRICT pagemappings2=pagemappings;
      FreePageHeader *RESTRICT freepage2=freepage;
      for(n=0; n<sizeinpages; n++, pagemappings2++, freepage2=(FreePageHeader *RESTRICT)((size_t)freepage2 + PAGE_SIZE))
      {
        if(*pagemappings && !USERMODEPAGEALLOCATOR_ISPAGEFREE(*pagemappings, freepage2))
        { /* We're going to be exclusively writing to it */
          PREFETCHCACHELINE(freepage2, 1, 0);
        }
      }
    }
#endif
    for(n=0; n<sizeinpages; n++, pagemappings++, freepage=(FreePageHeader *RESTRICT)((size_t)freepage + PAGE_SIZE))
    {
      if(*pagemappings && !USERMODEPAGEALLOCATOR_ISPAGEFREE(*pagemappings, freepage))
      {
        *pagemappings=USERMODEPAGEALLOCATOR_SETPAGEFREEBIT(*pagemappings);
        freepage->magic=FREEPAGEHEADERMAGIC;
        freepage->older=addr->newestdirty;
        freepage->newer=0;
        freepage->age=addr->opcount;
        freepage->dirty=1;
        if(addr->newestdirty)
        {
          assert(!addr->newestdirty->newer);
          addr->newestdirty->newer=freepage;
        }
        else
          addr->oldestdirty=freepage;
        addr->newestdirty=freepage;
        addr->freepages++;
        addr->usedpages--;
        ValidateFreePageLists(addr);
      }
    }
#if 0
    /* Do I need to return memory to the system? */
    if((dofreesystemmemorycheck=!(addr->opcount++ & (USERMODEPAGEALLOCATOR_SYSTEMFREEMEMORYCHECKRATE-1))) || addr->freepages>USERMODEPAGEALLOCATOR_FREEPAGECACHESIZE(addr->usedpages, addr->freepages))
    {
      FreePageHeader *RESTRICT freepage;
      size_t pagestofree=USERMODEPAGEALLOCATOR_FREEPAGECACHESIZE(addr->usedpages, addr->freepages)<=addr->freepages ? addr->freepages-USERMODEPAGEALLOCATOR_FREEPAGECACHESIZE(addr->usedpages, addr->freepages) : 0, pagesmustfree=0;
      static double memorypressurescale=0;
      if(dofreesystemmemorycheck)
        memorypressurescale=mypow8(OSSystemMemoryPressure());
      pagesmustfree=(size_t)(addr->freepages*memorypressurescale);
      pagestofree+=pagesmustfree;
      /* Don't bother with the overhead of freeing memory unless you have a sizeable chunk to do at once */
      if(pagestofree>=REMAPMEMORYPAGESBLOCKSIZE)
      {
        int done=0;
        RemapMemoryPagesBlock memtodecommit;
        memtodecommit.idx=0;
        /* Try to cull dirty before clean pages if poss */
        for(freepage=addr->oldestdirty; !done; freepage=addr->oldestdirty, pagestofree--, pagesmustfree=pagesmustfree ? pagesmustfree-1 : 0)
        {
          size_t freepagepfidx;
          if(!freepage || !pagestofree || (!pagesmustfree && addr->opcount-freepage->age<USERMODEPAGEALLOCATOR_FREEPAGECACHEAGE(addr->usedpages, addr->freepages)))
          {
            done=1;
            goto decommitpages;
          }
          assert(FREEPAGEHEADERMAGIC==freepage->magic);
          assert(!freepage->older);
          addr->oldestdirty=freepage->newer;
          if(addr->oldestdirty)
            addr->oldestdirty->older=0;
          else
            addr->newestdirty=0;
          assert(!addr->oldestclean); /* clean pages not implemented yet */
          addr->freepages--;
          freepagepfidx=((size_t)freepage-(size_t)addr)/PAGE_SIZE;
          memtodecommit.addrs[memtodecommit.idx]=freepage;
          memtodecommit.pageframes[memtodecommit.idx]=USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(addr->pagemapping[freepagepfidx]);
          addr->pagemapping[freepagepfidx]=0;
          memtodecommit.idx++;
          ValidateFreePageLists(addr);

          if(REMAPMEMORYPAGESBLOCKSIZE==memtodecommit.idx)
          { /* Not actually needed, and it interferes with physical page emulation.
            OSRemapMemoryPagesOntoAddrs(memtodecommit.addrs, memtodecommitidx, memtodecommit.pageframes, &addr->OSreservedata);*/
decommitpages:
            if(memtodecommit.idx)
            {
              if(memtodecommit.idx==OSReleaseMemoryPages(memtodecommit.pageframes, memtodecommit.idx, &addr->OSreservedata))
              {
                memtodecommit.idx=0;
                ValidatePageMappings(addr);
              }
              else
              {
                size_t n;
                for(n=0; n<memtodecommit.idx; n++)
                {
                  freepage=(FreePageHeader *RESTRICT) memtodecommit.addrs[n];
                  freepagepfidx=((size_t)freepage-(size_t)addr)/PAGE_SIZE;
                  addr->pagemapping[freepagepfidx]=memtodecommit.pageframes[n];
                  freepage->magic=FREEPAGEHEADERMAGIC;
                  freepage->older=0;
                  freepage->newer=addr->oldestdirty;
                  if(addr->oldestdirty)
                    addr->oldestdirty->older=freepage;
                  else
                    addr->newestdirty=freepage;
                  addr->oldestdirty=freepage;
                  addr->freepages++;
                  ValidateFreePageLists(addr);
                }
                pagestofree+=memtodecommit.idx;
              }
            }
            break;
          }
        }
      }
    }
#endif
    if(!dontfreeVA)
    {
      if((size_t) addr->frontptr-size==(size_t) mem || (size_t) addr->backptr==(size_t) mem)
      {
        if(fromback)
          addr->backptr=(void *)((size_t) addr->backptr + size);
        else
          addr->frontptr=(void *)((size_t) addr->frontptr - size);
        /*if(!addr->pagesused)
            CheckFreeAddressSpaces(&addressspacereservation); */
        return 1;
      }
    }
    return 0;
  }
  return 0;
}

static int SwapPages(void *dest, void *start, void *end)
{
  int destfromback;
  AddressSpaceReservation_t *destaddr=AddressSpaceFromMem(&destfromback, dest), *srcaddr=AddressSpaceFromMem(0, start);
  size_t pages, n, detachedfreepages1, detachedfreepages2, destpfidx=((size_t)dest-(size_t)destaddr)/PAGE_SIZE, srcpfidx=((size_t)start-(size_t)srcaddr)/PAGE_SIZE, endpfidx=((size_t)end-(size_t)srcaddr)/PAGE_SIZE;
  PageFrameType *RESTRICT destpf=destaddr->pagemapping+destpfidx, *RESTRICT srcpf=srcaddr->pagemapping+srcpfidx, *RESTRICT endpf=srcaddr->pagemapping+endpfidx;
  FreePageHeader *RESTRICT destfreepage, *RESTRICT srcfreepage;
  pages=((size_t)end-(size_t)start)/PAGE_SIZE;
  destfreepage=(FreePageHeader *RESTRICT) dest;
  srcfreepage=(FreePageHeader *RESTRICT) start;
  detachedfreepages1=0;
  for(n=0; n<pages; n++, destfreepage=(FreePageHeader *RESTRICT)((size_t) destfreepage + PAGE_SIZE), srcfreepage=(FreePageHeader *RESTRICT)((size_t) srcfreepage + PAGE_SIZE))
  {
    PageFrameType t;
    int srcIsFree=srcpf[n] && USERMODEPAGEALLOCATOR_ISPAGEFREE(srcpf[n], srcfreepage);
    int destIsFree=destpf[n] && USERMODEPAGEALLOCATOR_ISPAGEFREE(destpf[n], destfreepage);
    assert(srcpf[n]);
    assert(!srcIsFree);
    if((srcIsFree && destIsFree) || (!srcIsFree && !destIsFree))
    {
      /* Do nothing, as either it doesn't matter or pointers come out the same */
    }
    else
    {
      AddressSpaceReservation_t *RESTRICT addr=srcIsFree ? srcaddr : destaddr;
      PageFrameType *RESTRICT pf=srcIsFree ? srcpf : destpf;
      FreePageHeader *RESTRICT freepage=srcIsFree ? srcfreepage : destfreepage;
      DetachFreePage(addr, pf, freepage);
      freepage->older=freepage->newer=0;
      detachedfreepages1++;
    }
    /*printf("Swapping page frames %p (%p) and %p (%p)\n", srcpf[n], srcfreepage, destpf[n], destfreepage);*/
    t=destpf[n];
    destpf[n]=USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(srcpf[n]);
    srcpf[n]=USERMODEPAGEALLOCATOR_MASKOUTPAGEFREEBIT(t);
  }
  /* This bug in MapUserPhysicalPagesScatter() where it sometimes requires the pages being
  mapped to be demapped first in a *separate* call is painful :( */
  OSRemapMemoryPagesOntoAddr(start, pages, NULL, &destaddr->OSreservedata);
  OSRemapMemoryPagesOntoAddr(dest, pages, NULL, &destaddr->OSreservedata);
  /* Only the pages from start to end are guaranteed to exist. Those at dest may be patchy,
  so we batch those. */
  {
    int done=0;
    void *start2=start;
    PageFrameType *RESTRICT srcpf2=srcpf;
    RemapMemoryPagesBlock memtocommit;
    memtocommit.idx=0;
    for(n=0; !done; n++, start2=(void *)((size_t) start2 + PAGE_SIZE), srcpf2++)
    {
      if(n>=pages)
      {
        done=1;
        goto commitpages;
      }
      if(*srcpf2)
      {
        memtocommit.addrs[memtocommit.idx]=start2;
        memtocommit.pageframes[memtocommit.idx]=*srcpf2;
        memtocommit.idx++;
      }
      if(REMAPMEMORYPAGESBLOCKSIZE==memtocommit.idx)
      {
commitpages:
        if(memtocommit.idx)
        {
          OSRemapMemoryPagesOntoAddr(memtocommit.addrs, memtocommit.idx, memtocommit.pageframes, &destaddr->OSreservedata);
          memtocommit.idx=0;
        }
      }
    }
  }
  OSRemapMemoryPagesOntoAddr(dest, pages, destpf, &destaddr->OSreservedata);
  destfreepage=(FreePageHeader *RESTRICT) dest;
  srcfreepage=(FreePageHeader *RESTRICT) start;
  detachedfreepages2=0;
  for(n=0; n<pages; n++, destfreepage=(FreePageHeader *RESTRICT)((size_t) destfreepage + PAGE_SIZE), srcfreepage=(FreePageHeader *RESTRICT)((size_t) srcfreepage + PAGE_SIZE))
  {
    int srcIsFree=srcpf[n] && FREEPAGEHEADERMAGIC==srcfreepage->magic;
    int destIsFree=destpf[n] && FREEPAGEHEADERMAGIC==destfreepage->magic;
    if(srcIsFree) srcpf[n]=USERMODEPAGEALLOCATOR_SETPAGEFREEBIT(srcpf[n]);
    if(destIsFree) destpf[n]=USERMODEPAGEALLOCATOR_SETPAGEFREEBIT(destpf[n]);
    if((srcIsFree && destIsFree) || (!srcIsFree && !destIsFree))
    {
      /* Do nothing, as either it doesn't matter or pointers come out the same */
    }
    else
    {
      AddressSpaceReservation_t *RESTRICT addr=srcIsFree ? srcaddr : destaddr;
      PageFrameType *RESTRICT pf=srcIsFree ? srcpf : destpf;
      FreePageHeader *RESTRICT freepage=srcIsFree ? srcfreepage : destfreepage;
      assert(freepage->dirty);
      assert(!freepage->older);
      assert(!freepage->newer);
      freepage->older=addr->newestdirty;
      freepage->newer=0;
      freepage->age=addr->opcount;
      freepage->dirty=1;
      if(addr->newestdirty)
      {
        assert(!addr->newestdirty->newer);
        addr->newestdirty->newer=freepage;
      }
      else
        addr->oldestdirty=freepage;
      addr->newestdirty=freepage;
      detachedfreepages2++;
    }
  }
  assert(detachedfreepages1==detachedfreepages2);
  srcaddr->usedpages-=pages;
  destaddr->usedpages+=pages;
  ValidateFreePageLists(destaddr);
  ValidatePageMappings(destaddr);
  return 1;
}

/************************************************************************************/

#ifdef USERMODEPAGEALLOCATOR_DEBUGCONFIG
#define REGIONSTORAGESIZE PAGE_SIZE
#else
#define REGIONSTORAGESIZE (PAGE_SIZE*4)
#endif
#define REGIONSPERSTORAGE ((REGIONSTORAGESIZE-3*sizeof(void *))/sizeof(region_node_t))
typedef struct RegionStorage_s RegionStorage_t;
static struct RegionStorage_s
{
  size_t magic;
  RegionStorage_t *next;
  region_node_t *freeregions;
  char padding[REGIONSTORAGESIZE-(3*sizeof(void *)+REGIONSPERSTORAGE*sizeof(region_node_t))];
  region_node_t regions[REGIONSPERSTORAGE];
} *regionstorage;
static size_t regionsallocated, regionsfree;

static region_node_t *AllocateRegionNode(void)
{
  int n;
  if(!regionstorage || !regionstorage->freeregions)
  {
    RegionStorage_t **_rs=!regionstorage ? &regionstorage : &regionstorage->next, *rs;
    if(regionstorage) while(*_rs) _rs=&((*_rs)->next);
    assert(128==sizeof(region_node_t) || 64==sizeof(region_node_t));
    assert(sizeof(RegionStorage_t)<=REGIONSTORAGESIZE);
    assert(!((size_t) (*_rs)->regions & (sizeof(region_node_t)-1)));
    if((rs=*_rs=(RegionStorage_t *) AllocatePages(0, REGIONSTORAGESIZE, USERPAGE_TOPDOWN)))
    {
#ifdef DEBUG
      printf("RegionStorage extends %p to %p\n", rs, ((size_t)rs+REGIONSTORAGESIZE));
#endif
#if !MMAP_CLEAR
      memset(rs, 0, REGIONSTORAGESIZE);
#endif
      rs->magic=*(size_t *)"UMPARSTO";
      for(n=REGIONSPERSTORAGE-1; n>=0; n--)
      {
        *(region_node_t **)&rs->regions[n]=regionstorage->freeregions;
        regionstorage->freeregions=&rs->regions[n];
      }
      regionsallocated+=REGIONSPERSTORAGE;
      regionsfree+=REGIONSPERSTORAGE;
    }
  }
  if(regionstorage->freeregions)
  {
    region_node_t *ret=regionstorage->freeregions;
#ifdef DEBUG
    {
      RegionStorage_t *t;
      for(t=regionstorage; t; t=t->next)
      {
        assert(*(size_t *)"UMPARSTO"==t->magic);
      }
    }
#endif
    regionstorage->freeregions=*(region_node_t **)ret;
    *(region_node_t **)ret=0;
    regionsfree--;
    return ret;
  }
  return 0;
}
static int CheckFreeRegionNodes(RegionStorage_t **_rs)
{
  if(regionsfree==regionsallocated)
  {
    if(!(*_rs)->next || CheckFreeRegionNodes(&(*_rs)->next))
    {
      assert(!(*_rs)->next);
#ifdef DEBUG
      printf("RegionStorage releases %p to %p\n", *_rs, ((size_t)(*_rs)+REGIONSTORAGESIZE));
#endif
      ReleasePages(*_rs, REGIONSTORAGESIZE, 0);
      *_rs=0;
      regionsallocated=regionsfree=0;
      return 1;
    }
  }
  return 0;
}
static void FreeRegionNode(region_node_t *node)
{
  memset(node, 0, sizeof(region_node_t));
  *(region_node_t **)node=regionstorage->freeregions;
  regionstorage->freeregions=node;
  regionsfree++;
}

#if USE_LOCKS
static MLOCK_T userpagemutex;
#endif

static void userpage_validatestate(MemorySource *source)
{
#ifndef NDEBUG
  region_node_t *RESTRICT r, *RESTRICT rfree;
  int fromback=0;
#ifdef DEBUG
    {
      RegionStorage_t *t;
      for(t=regionstorage; t; t=t->next)
      {
        assert(*(size_t *)"UMPARSTO"==t->magic);
      }
    }
#endif
  NEDTRIE_FOREACH(rfree, regionL_tree_s, &source->regiontreeL)
  {
    AddressSpaceReservation_t *RESTRICT addr=AddressSpaceFromMem(&fromback, rfree->start);
    size_t n, pagepfidx=((size_t)rfree->start-(size_t)addr)/PAGE_SIZE;
    FreePageHeader *RESTRICT freepage=(FreePageHeader *RESTRICT)rfree->start;
    assert(rfree->end>rfree->start);
    /* Ensure that every free block is also allocated */
    assert(REGION_HASNODEHEADER(regionA_tree_s, rfree, linkA));
    r=REGION_FIND(regionA_tree_s, &source->regiontreeA, rfree);
    assert(rfree==r);
    /* Ensure that every free block never has another free block preceding or postceding it */
    if(rfree->prev)
    {
      assert(!REGION_HASNODEHEADER(regionL_tree_s, rfree->prev, linkL));
    }
    /* Ensure that this free block lives in an address reservation */
    assert(addr);
    /* Ensure that any pages in this free block are definitely marked as free */
    for(n=pagepfidx; n<((size_t)rfree->end-(size_t)addr)/PAGE_SIZE; n++, freepage=(FreePageHeader *RESTRICT)((size_t) freepage + PAGE_SIZE))
    {
      if(addr->pagemapping[n])
      {
        assert(FREEPAGEHEADERMAGIC==freepage->magic);
      }
    }
  }
  if(!source->firstregion)
  {
    assert(!source->lastregion);
  }
  else
  {
#if 0
    /* The entry on HEAD must never be a free chunk */
    assert(!REGION_HASNODEHEADER(regionL_tree_s, source->lastregion, linkL));
#endif
    for(r=source->firstregion; r; r=r->next)
    {
      int mode;
      AddressSpaceReservation_t *RESTRICT addr=AddressSpaceFromMem(&fromback, r->start);
      size_t n, pagepfidx=((size_t)r->start-(size_t)addr)/PAGE_SIZE;
      FreePageHeader *RESTRICT freepage=(FreePageHeader *RESTRICT)r->start;
      assert(r->end>r->start);
      /* Ensure that this block lives in an address reservation */
      assert(addr);
      assert((r->start>=addr->front && r->end<=addr->frontptr) || (r->start>=addr->backptr && r->end<=addr->back));
      /* Ensure every item is in the allocated list. */
      rfree=REGION_FIND(regionA_tree_s, &source->regiontreeA, r);
      assert(rfree==r);
      /* Ensure contiguity. This test always fails if there is more than one
      address space reservation. */
      if(fromback)
      {
        assert(!source->firstregion->next);
        assert(!source->lastregion->prev);
      }
      else
      {
        assert(!source->firstregion->prev);
        assert(!source->lastregion->next);
      }
      if(r->prev)
      {
        assert(r->prev->next==r);
        assert(r->prev->end==r->start);
      }
      else
      {
        assert(fromback ? source->lastregion==r : source->firstregion==r);
      }
      if(r->next)
      {
        assert(r->next->prev==r);
        assert(r->next->start==r->end);
      }
      else
      {
        assert(fromback ? source->firstregion==r : source->lastregion==r);
      }
      /* Ensure that any pages in this block are contiguously allocated */
      mode=0;
      for(n=pagepfidx; n<((size_t)r->end-(size_t)addr)/PAGE_SIZE; n++, freepage=(FreePageHeader *RESTRICT)((size_t) freepage + PAGE_SIZE))
      {
        if(!mode)
        { /* mode=0: Look for initial range of allocated pages */
          if(!n) { assert(addr->pagemapping[n]); }
          if(addr->pagemapping[n])
          {
            if(FREEPAGEHEADERMAGIC==freepage->magic) mode=1;
          }
          else mode=1;
        }
        else if(1==mode)
        { /* mode=1: All pages from now on must be free or not allocated */
          if(addr->pagemapping[n])
          {
            assert(FREEPAGEHEADERMAGIC==freepage->magic);
          }
        }
      }
    }
  }
#endif
}

static void AddRegionNode(MemorySource *source, region_node_t *RESTRICT newnode, int fromback)
{
  REGION_INSERT(regionA_tree_s, &source->regiontreeA, newnode);
  if(fromback)
  {
    newnode->prev=0;
    newnode->next=source->lastregion;
    if(source->lastregion)
      source->lastregion->prev=newnode;
    else
      source->firstregion=newnode;
    source->lastregion=newnode;
  }
  else
  {
    newnode->prev=source->lastregion;
    newnode->next=0;
    if(source->lastregion)
      source->lastregion->next=newnode;
    else
      source->firstregion=newnode;
    source->lastregion=newnode;
  }
}
/* Special flags: USERPAGE_TOPDOWN causes the allocation to be made from the top down.
USERPAGE_NOCOMMIT causes no memory to be committed */
static void *userpage_malloc(size_t toallocate, unsigned flags)
{
  void *ret=0;
  region_node_t node, *RESTRICT r, *RESTRICT newnode=0;
  MemorySource *source=(flags & USERPAGE_TOPDOWN) ? &upper : &lower;
  unsigned mremapvalue = (flags & M2_RESERVE_MASK)>>8;
  size_t size = mremapvalue ? ((flags & M2_RESERVE_ISMULTIPLIER) ? toallocate*mremapvalue : (size_t)1<<mremapvalue) : toallocate;
  if(size < toallocate)
    size = toallocate;
  /* Firstly find out if there is a free slot of sufficient size and if so use that.
  If there isn't a sufficient free slot, extend the virtual address space */
  node.start=0;
  node.end=(void *)size;
#if USE_LOCKS
  ACQUIRE_LOCK(&userpagemutex);
#endif
  r=REGION_NFIND(regionL_tree_s, &source->regiontreeL, &node);
  if(r)
  {
    size_t rlen=(size_t) r->end - (size_t) r->start;
    if(rlen<size)
    {
      assert(rlen>=size);
      abort();
    }
    REGION_REMOVE(regionL_tree_s, &source->regiontreeL, r);
    memset(&r->linkL, 0, sizeof(r->linkL));
    if(rlen==size)
    { /* Don't need to split */
      ret=r->start;
      goto commitpages;
    }
    /* Split r with new start addr */
    if(!(newnode=AllocateRegionNode()))
      goto mfail;
    ret=r->start;
    newnode->start=(void *)((size_t) r->start + size);
    newnode->end=r->end;
    r->end=newnode->start;
    newnode->prev=r;
    newnode->next=r->next;
    if(newnode->next) newnode->next->prev=newnode;
    else if(flags & USERPAGE_TOPDOWN)
    {
      assert(r==source->firstregion);
      source->firstregion=newnode;
    }
    r->next=newnode;
    REGION_INSERT(regionA_tree_s, &source->regiontreeA, newnode);
    REGION_INSERT(regionL_tree_s, &source->regiontreeL, newnode);
  }
  else
  { /* Reserve sufficient new address space */
    if(!(newnode=AllocateRegionNode()))
      goto mfail;
    if(!(ret=AllocatePages(0, size, USERPAGE_NOCOMMIT|flags)))
      FreeRegionNode(newnode);
    else
    {
      newnode->start=ret;
      newnode->end=(void *)((size_t) ret + size);
      if(source->lastregion && source->lastregion->start!=newnode->end)
      { /* If this happens, it is because RegionStorage has extended itself.
        To handle this situation, insert a dummy allocated block. */
        region_node_t *RESTRICT dummy=AllocateRegionNode();
        if(!dummy)
        {
          ReleasePages(ret, size, 0);
          FreeRegionNode(newnode);
          ret=0;
          goto mfail;
        }
        dummy->start=newnode->end;
        dummy->end=source->lastregion->start;
        AddRegionNode(source, dummy, flags & USERPAGE_TOPDOWN);
      }
      AddRegionNode(source, newnode, flags & USERPAGE_TOPDOWN);
    }
  }
commitpages:
  if(!ret) goto mfail;
  if(!(flags & USERPAGE_NOCOMMIT))
  {
    if(!AllocatePages(ret, toallocate, 0))
    {
      ReleasePages(ret, size, 0);
      goto mfail;
    }
    else if(flags & USERPAGE_TOPDOWN)
    {
#ifdef DEBUG
      size_t n, *RESTRICT p;
      for(n=0, p=(size_t *) ret; n<toallocate; n+=sizeof(size_t), p++)
      { /* All memory returned from upper must always be zeroed */
        assert(!*p);
      }
#endif
    }
  }
  userpage_validatestate(source);
#if USE_LOCKS
  RELEASE_LOCK(&userpagemutex);
#endif
  return ret;
mfail:
#if USE_LOCKS
  RELEASE_LOCK(&userpagemutex);
#endif
  return MFAIL;
}

static int userpage_free(void *mem, size_t size)
{
  region_node_t node, *RESTRICT r=0;
  int fromback, prevIsFree, nextIsFree;
  MemorySource *source=0;
  AddressSpaceReservation_t *RESTRICT addr=0;
  node.start=mem;
  node.end=0; /* size may actually be quite a lot smaller than region size */
#if USE_LOCKS
  ACQUIRE_LOCK(&userpagemutex);
#endif
  if((addr=AddressSpaceFromMem(&fromback, mem)))
    source=fromback ? &upper : &lower;
  else
    goto fail;
  r=REGION_FIND(regionA_tree_s, &source->regiontreeA, &node);
  if(!r)
  { /* Block not found */
    assert(r);
    goto fail;
  }
  /* Can I merge with adjacent free blocks? */
  prevIsFree=r->prev && REGION_HASNODEHEADER(regionL_tree_s, r->prev, linkL);
  nextIsFree=r->next && REGION_HASNODEHEADER(regionL_tree_s, r->next, linkL);
  if(nextIsFree)
  { /* Consolidate into r */
    region_node_t *RESTRICT t;
    if(!fromback)
    {
      assert(r->next->start==r->end);
      assert(r->next->prev==r);
    }
    REGION_REMOVE(regionA_tree_s, &source->regiontreeA, r->next);
    REGION_REMOVE(regionL_tree_s, &source->regiontreeL, r->next);
    t=r->next;
    r->end=t->end;
    r->next=t->next;
    if(r->next) r->next->prev=r;
    if(fromback && source->firstregion==t)
    {
      source->firstregion=r;
      assert(!r->next);
    }
    FreeRegionNode(t);
  }
  if(prevIsFree)
  { /* Consolidate into prev */
    region_node_t *RESTRICT t;
    if(!fromback)
    {
      assert(r->prev->end==r->start);
      assert(r->prev->next==r);
    }
    t=r->prev;
    REGION_REMOVE(regionA_tree_s, &source->regiontreeA, r);
    REGION_REMOVE(regionL_tree_s, &source->regiontreeL, t);
    t->end=r->end;
    t->next=r->next;
    if(t->next) t->next->prev=t;
    if(fromback && source->firstregion==r)
    {
      source->firstregion=t;
      assert(!t->next);
    }
    FreeRegionNode(r);
    r=t;
  }
  assert(r->end>r->start);
  if(!ReleasePages(r->start, (size_t)r->end - (size_t)r->start, 0))
  { /* We didn't shrink VA, so add newly freed region to free list */
    REGION_INSERT(regionL_tree_s, &source->regiontreeL, r);
  }
  else
  { /* We did shrink VA, so remove newly freed region from allocated list */
    REGION_REMOVE(regionA_tree_s, &source->regiontreeA, r);
    assert(source->lastregion==r);
    if(fromback)
    {
      source->lastregion=r->next;
      if(source->lastregion)
        source->lastregion->prev=0;
      else
        source->firstregion=0;
    }
    else
    {
      source->lastregion=r->prev;
      if(source->lastregion)
        source->lastregion->next=0;
      else
        source->firstregion=0;
    }
    FreeRegionNode(r);
  }
  userpage_validatestate(source);
#if USE_LOCKS
  RELEASE_LOCK(&userpagemutex);
#endif
  return 0;
fail:
#if USE_LOCKS
  RELEASE_LOCK(&userpagemutex);
#endif
  return -1;
}

static void *userpage_realloc(void *mem, size_t oldsize, size_t newsize, int flags, unsigned flags2)
{
  void *ret=0;
  region_node_t node, *RESTRICT r=0;
  int fromback;
  MemorySource *source=0;
  AddressSpaceReservation_t *RESTRICT addr=0;
  size_t regionsize;
  node.start=mem;
  node.end=(void *)((size_t) mem + newsize); /* not actually used for the search */
#if USE_LOCKS
  ACQUIRE_LOCK(&userpagemutex);
#endif
  if((addr=AddressSpaceFromMem(&fromback, mem)))
    source=fromback ? &upper : &lower;
  else
    goto mfail;
  r=REGION_FIND(regionA_tree_s, &source->regiontreeA, &node);
  if(!r)
  { /* Block not found */
    assert(r);
    goto mfail;
  }
  regionsize=(size_t) r->end - (size_t) r->start;
  if(node.end>r->end)
  { /* We ordinarily have the region's size available to us, but if he wants
    more then can we expand? */
    if(r->next && REGION_HASNODEHEADER(regionL_tree_s, r->next, linkL))
    {
      region_node_t *RESTRICT t;
      if(node.end>r->next->end)
        goto relocate;
      /* Consolidate into r */
      assert(r->next->start==r->end);
      assert(r->next->prev==r);
      REGION_REMOVE(regionA_tree_s, &source->regiontreeA, r->next);
      REGION_REMOVE(regionL_tree_s, &source->regiontreeL, r->next);
      t=r->next;
      r->end=t->end;
      r->next=t->next;
      if(r->next) r->next->prev=r;
      FreeRegionNode(t);
      /* Fall through */
    }
    else if(r->end==addr->frontptr)
    {
      if((size_t)addr->backptr - (size_t) addr->frontptr < newsize-regionsize)
        goto relocate;
      /* Extend VA and r */
      r->end=addr->frontptr=(void *)((size_t) addr->frontptr + newsize - regionsize);
      /* Fall through */
    }
    else
      goto relocate;
  }
  if(newsize>oldsize)
  { /* Ensure there are pages up to newsize, but you need to be careful if you're extending VA */
    void *newmemaddr=(void *)((size_t) mem + oldsize);
    size_t newmemsize=newsize-oldsize;
    assert((void *)((size_t) mem + newsize)<=r->end);
    if(!AllocatePages(newmemaddr, newmemsize, flags2))
      goto mfail;
  }
  else if(newsize<oldsize)
  { /* Free anything after newsize up to r->end */
    ReleasePages(node.end, (size_t) r->end - (size_t) node.end, 1);
  }
  userpage_validatestate(source);
#if USE_LOCKS
  RELEASE_LOCK(&userpagemutex);
#endif
  return mem;
relocate:
  if(!(flags & MREMAP_MAYMOVE)) goto mfail;
  if(MFAIL==(ret=userpage_malloc(newsize, flags2|USERPAGE_NOCOMMIT|(fromback ? USERPAGE_TOPDOWN : 0)))) goto mfail;
  if(newsize>oldsize)
  { /* Ensure there are pages from oldsize to newsize */
    void *newmemaddr=(void *)((size_t) ret + oldsize);
    if(!AllocatePages(newmemaddr, newsize-oldsize, 0))
    {
      userpage_free(ret, newsize);
      goto mfail;
    }
#ifdef DEBUG
    /*printf("Allocated new pages between %p and %p\n", newmemaddr, (void *)((size_t) newmemaddr + newsize - oldsize));*/
#endif
  }
#ifdef DEBUG
  /*printf("Swapping pages from %p-%p to %p-%p\n", r->start, (void *)((size_t) r->start + oldsize), ret, (void *)((size_t) ret + oldsize));*/
#endif
  if(!SwapPages(ret, r->start, (void *)((size_t) r->start + oldsize)))
  {
    userpage_free(ret, newsize);
    goto mfail;
  }
  userpage_free(r->start, (size_t) r->end - (size_t) r->start);
#if USE_LOCKS
  RELEASE_LOCK(&userpagemutex);
#endif
  return ret;
mfail:
#if USE_LOCKS
  RELEASE_LOCK(&userpagemutex);
#endif
  return MFAIL;
}

#endif
